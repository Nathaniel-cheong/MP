{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b688066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit run streamlit_site/streamlit_app.py\n",
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sqlalchemy import (\n",
    "    select, create_engine, text, Table, Column, Integer, String, MetaData, ForeignKey, LargeBinary)\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import pdfplumber\n",
    "import fitz  # PyMuPDF\n",
    "# For Image Display within the df\n",
    "from IPython.display import Image\n",
    "from PIL import Image, ImageOps\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a6120cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intializing database connection\n",
    "# Replace values with your actual database info\n",
    "# username = \"mpams_user\"\n",
    "# password = \"qmy7aT6NQSuAeMNfY02w7fAGjJ7lXqBm\"\n",
    "# host = \"dpg-d1hk6bbipnbc73f4du6g-a.singapore-postgres.render.com\"\n",
    "# port = \"5432\"\n",
    "# database = \"mpams\"\n",
    "\n",
    "# # SQLAlchemy connection URL\n",
    "# DATABASE_URL = f\"postgresql://{username}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "host = \"aws-0-ap-southeast-1.pooler.supabase.com\"\n",
    "port = \"5432\"\n",
    "database = \"postgres\"\n",
    "username = \"postgres.thqqtxvmzisznglpukwh\"\n",
    "password = \"ImehQhjJwRw2wnkO\"\n",
    "\n",
    "# SQLAlchemy connection URL\n",
    "DATABASE_URL = f\"postgresql://{username}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "# Create engine\n",
    "engine = create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2afcf80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the 2 tables\n",
    "metadata = MetaData()\n",
    "\n",
    "pdf_info = Table(\n",
    "    \"pdf_info\", metadata,\n",
    "    Column(\"pdf_id\", String, primary_key=True),\n",
    "    Column(\"year\", Integer, nullable=False),\n",
    "    Column(\"brand\", String, nullable=False),\n",
    "    Column(\"model\", String, nullable=False),\n",
    "    Column(\"batch_id\", String, nullable=False),\n",
    "    Column(\"bike_image\", LargeBinary),\n",
    ")\n",
    "\n",
    "master_parts_list = Table(\n",
    "    \"master_parts_list\", metadata,\n",
    "    Column(\"mpl_id\", Integer, primary_key=True, autoincrement=True),\n",
    "    Column(\"part_no\", String, nullable=False),\n",
    "    Column(\"description\", String, nullable=False),\n",
    "    Column(\"ref_no\", Integer, nullable=False),\n",
    "    Column(\"add_info\", String),\n",
    "    Column(\"section_id\", String, ForeignKey(\"pdf_section.section_id\", ondelete=\"CASCADE\")),\n",
    "    Column(\"pdf_id\", String, ForeignKey(\"pdf_info.pdf_id\", ondelete=\"CASCADE\")),\n",
    ")\n",
    "\n",
    "pdf_section = Table(\n",
    "    \"pdf_section\", metadata,\n",
    "    Column(\"section_id\", String, primary_key=True),\n",
    "    Column(\"section_no\", String, nullable=False),\n",
    "    Column(\"section_name\", String, nullable=False),\n",
    "    Column(\"cc\", String, nullable=False),\n",
    "    Column(\"section_image\", LargeBinary, nullable=False),\n",
    "    Column(\"pdf_id\", String, ForeignKey(\"pdf_info.pdf_id\", ondelete=\"CASCADE\")),\n",
    ")\n",
    "\n",
    "pdf_log = Table(\n",
    "    \"pdf_log\", metadata,\n",
    "    Column(\"log_id\", Integer, primary_key=True, autoincrement=True),\n",
    "    Column(\"pdf_id\", String, ForeignKey(\"pdf_info.pdf_id\", ondelete=\"CASCADE\")),\n",
    "    Column(\"account_id\", String, nullable=False), # add FK next time\n",
    "    Column(\"timestamp\", String, nullable=False),\n",
    "    Column(\"is_active\", Integer, nullable=False),\n",
    "    Column(\"is_current\", Integer, nullable=False),\n",
    ")\n",
    "\n",
    "# Create the tables\n",
    "metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code (For Reference)\n",
    "# ---------- HELPERS ----------\n",
    "def extract_pdf_id(pdf_path):\n",
    "    # To-Do: modify pdf_id extraction to suit both pdf format\n",
    "    base_filename = os.path.basename(pdf_path).split('.')[0]\n",
    "    match = re.match(r\"([A-Za-z0-9 ]+)\", base_filename)\n",
    "    if match:\n",
    "        return match.group(1).replace(\" \", \"\")  # Remove all spaces\n",
    "    return None\n",
    "\n",
    "def extract_year(pdf_path):\n",
    "    year_match = re.search(r\"'(\\d{2})\", pdf_path)\n",
    "    return f\"20{year_match.group(1)}\" if year_match else None\n",
    "\n",
    "def extract_model(pdf_path):\n",
    "    base_filename = os.path.basename(pdf_path)\n",
    "    match = re.search(r\"\\((.*?)\\)\", base_filename)\n",
    "    if match:\n",
    "        return match.group(1)  # e.g., \"B65P, B65R, B65S\"\n",
    "    return None\n",
    "\n",
    "# ---------- IMAGE EXTRACTION ----------\n",
    "def normalize_image_background(image_bytes):\n",
    "    img = Image.open(BytesIO(image_bytes)).convert(\"L\")  # Grayscale\n",
    "    mean_brightness = sum(img.getdata()) / (img.width * img.height)\n",
    "\n",
    "    if mean_brightness < 128:  # Invert if it's a dark background\n",
    "        img = ImageOps.invert(img)\n",
    "\n",
    "    img = img.convert(\"RGB\")  # Convert back to RGB\n",
    "\n",
    "    output = BytesIO()\n",
    "    img.save(output, format=\"PNG\")\n",
    "    return output.getvalue()\n",
    "\n",
    "def get_existing_fig_combos(engine, pdf_id):\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(\n",
    "            text(\"SELECT section FROM parts_images WHERE pdf_id = :pdf_id\"),\n",
    "            {\"pdf_id\": pdf_id}\n",
    "        )\n",
    "        return set(str(row[0]) for row in result.fetchall())  # Cast to str for consistent comparison\n",
    "\n",
    "def extract_images_with_fig_labels(pdf_path, pdf_id, engine):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    data = []\n",
    "\n",
    "    # Step 1: Get existing (pdf_id, fig_no) combos from DB\n",
    "    existing_figs = get_existing_fig_combos(engine, pdf_id)\n",
    "\n",
    "    seen_figs = set()  # Track unique figs within the PDF\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "\n",
    "        matches = re.findall(r\"FIG\\.\\s*([\\w-]+)\", text)\n",
    "        if not matches:\n",
    "            continue\n",
    "\n",
    "        section = matches[0]\n",
    "\n",
    "        if section in seen_figs or section in existing_figs:\n",
    "            continue  # Skip if already handled or exists in DB\n",
    "\n",
    "        image_list = page.get_images(full=True)\n",
    "        if not image_list:\n",
    "            continue\n",
    "\n",
    "        xref = image_list[0][0]\n",
    "        base_image = doc.extract_image(xref)\n",
    "        image = normalize_image_background(base_image[\"image\"])\n",
    "\n",
    "        image_id = \"_\".join([pdf_id, section])\n",
    "\n",
    "        data.append({\n",
    "            \"image_id\" : image_id,\n",
    "            \"pdf_id\": pdf_id,\n",
    "            \"section\": section,\n",
    "            \"image\": image\n",
    "        })\n",
    "        seen_figs.add(section)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ---------- TEXT EXTRACTION ----------\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    all_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text() or \"\"\n",
    "            lines = text.split('\\n')\n",
    "            first_line = lines[0].strip() if lines else \"\"\n",
    "            if not any(\"FIG.\" in line for line in lines):\n",
    "                continue\n",
    "            if \"NUMERICAL INDEX\" in first_line:\n",
    "                break\n",
    "            all_text += f\"\\n--- Page {page_num + 1} ---\\n{text}\\n\"\n",
    "    return all_text\n",
    "\n",
    "def yamaha_process_data(text, pdf_id, year, model, num_model):\n",
    "    rows = []\n",
    "    lines = text.strip().split('\\n')\n",
    "    section = c_name = prev_fig_no = prev_c_name = prev_ref_no = \"\"\n",
    "    collect_data = False\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        if line.startswith('FIG.'):\n",
    "            tokens = line.split()\n",
    "            if len(tokens) >= 3:\n",
    "                section = tokens[1]\n",
    "                c_name = \" \".join(tokens[2:])\n",
    "                prev_fig_no, prev_c_name = section, c_name\n",
    "                collect_data = True\n",
    "            continue\n",
    "        if not collect_data: continue\n",
    "        if not section:\n",
    "            section, c_name = prev_fig_no, prev_c_name\n",
    "\n",
    "        parts = line.split()\n",
    "        is_valid_data_line = (\n",
    "            len(parts) >= 2 and \n",
    "            (re.match(r'\\w+[-–]\\w+', parts[0]) or parts[0].isdigit())\n",
    "        )\n",
    "        if not is_valid_data_line:\n",
    "            continue\n",
    "\n",
    "        if parts[0].isdigit():\n",
    "            ref_no = parts[0]\n",
    "            part_no = parts[1]\n",
    "            rest = parts[2:]\n",
    "            prev_ref_no = ref_no\n",
    "        else:\n",
    "            ref_no = prev_ref_no\n",
    "            part_no = parts[0]\n",
    "            rest = parts[1:]\n",
    "\n",
    "        rest = \" \".join(rest).split()\n",
    "        description = remarks = \"\"\n",
    "        numbers = []\n",
    "        found_numbers = False\n",
    "        for item in rest:\n",
    "            if item.isdigit():\n",
    "                numbers.append(item)\n",
    "                found_numbers = True\n",
    "                continue\n",
    "            if not found_numbers:\n",
    "                description += item + \" \"\n",
    "            else:\n",
    "                remarks += item + \" \"\n",
    "        if len(numbers) > num_model:\n",
    "            description += numbers[0]\n",
    "\n",
    "        image_id = \"_\".join([pdf_id, section])\n",
    "\n",
    "        rows.append([pdf_id, year, \"Yamaha\", model, section, c_name, ref_no, part_no, description.strip(), remarks.strip(), image_id])\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        'pdf_id', 'year', 'brand', 'model', 'section', 'component_name',\n",
    "        'ref_no', 'part_no', 'description', 'remarks', 'image_id'\n",
    "    ])\n",
    "\n",
    "# ---------- MAIN PROCESS ----------\n",
    "def yamaha_data_extraction(pdf_path):\n",
    "\n",
    "    pdf_id = extract_pdf_id(pdf_path)\n",
    "    year = extract_year(pdf_path)\n",
    "    model = extract_model(pdf_path)\n",
    "\n",
    "    SessionLocal = sessionmaker(bind=engine)\n",
    "    session = SessionLocal()\n",
    "    try:\n",
    "        # -------- pdf_log table --------\n",
    "        existing_log = session.execute(\n",
    "            select(1).select_from(pdf_log).where(pdf_log.c.pdf_id == pdf_id)\n",
    "        ).first()\n",
    "\n",
    "        if not existing_log:\n",
    "            session.execute(\n",
    "                pdf_log.insert().values(\n",
    "                    pdf_id=pdf_id,\n",
    "                    timestamp=datetime.now().isoformat()\n",
    "                )\n",
    "            )\n",
    "            session.commit()\n",
    "            print(f\"[INFO] Logged PDF '{pdf_id}' in pdf_log.\")\n",
    "        else:\n",
    "            print(f\"[INFO] PDF '{pdf_id}' already logged.\")\n",
    "\n",
    "        # -------- parts_images table --------\n",
    "        df_images = extract_images_with_fig_labels(pdf_path, pdf_id, engine)\n",
    "        image_message = f\"[INFO] Inserted {len(df_images)} new images for '{pdf_id}'.\"\n",
    "        if not df_images.empty:\n",
    "            df_images.to_sql(\"parts_images\", engine, if_exists=\"append\", index=False, method=\"multi\")\n",
    "            print(image_message)\n",
    "        else:\n",
    "            print(image_message + f\" All images for '{pdf_id}' already exist.\")\n",
    "\n",
    "        existing = session.execute(\n",
    "            select(1).select_from(master_parts_list).where(master_parts_list.c.pdf_id == pdf_id)\n",
    "        ).first()\n",
    "\n",
    "        if existing:\n",
    "            print(f\"[INFO] Master Parts data for '{pdf_id}' already exists.\")\n",
    "            return\n",
    "\n",
    "    finally:\n",
    "        session.close()\n",
    "            \n",
    "    # -------- master_parts_list table --------\n",
    "    all_text = extract_text_from_pdf(pdf_path)\n",
    "    df_parts = yamaha_process_data(all_text, pdf_id, year, model, num_model=3)\n",
    "\n",
    "    if not df_parts.empty:\n",
    "        #print(df_parts.to_string(index=False))\n",
    "        df_parts.to_sql(\"master_parts_list\", engine, if_exists=\"append\", index=False, method=\"multi\")\n",
    "        print(f\"[INFO] Inserted parts data for '{pdf_id}'.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Error, no parts data extracted for '{pdf_id}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0232a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code (For Refernece)\n",
    "# Format 1: Yamaha, Important images from page 6-60\n",
    "pdf_1 = \"Manuals/AEROX 155 '19 (B65P, B65R, B65S).pdf\"\n",
    "pdf_2= \"Manuals/FJR1300A '15 (1MCH, 1MCG).PDF\"\n",
    "# Format 2: Honda\n",
    "pdf_3 = \"Manuals/CRF1000 A_PC_13MJPG02_(G.H).pdf\"\n",
    "pdf_4 = \"Manuals/NC750XAP_13MKWM02_PC_2022_2023.pdf\"\n",
    "\n",
    "pdf_path = pdf_1\n",
    "\n",
    "brand = \"Yamaha\"\n",
    "supported_brands = ['Yamaha', 'Honda']\n",
    "\n",
    "if brand in supported_brands:\n",
    "    if brand == \"Yamaha\":\n",
    "        yamaha_data_extraction(pdf_path)\n",
    "    elif brand == \"Honda\":\n",
    "        print(\"Hi\")\n",
    "else:\n",
    "    print (f'\"{brand}\" not supported \\nAvailable Brands: {supported_brands}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fbcbdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B65P_B65R_B65S\n",
      "1MCH_1MCG\n",
      "13MJPG02\n",
      "13MKWM02\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# Format 1: Yamaha\n",
    "pdf_1 = \"Manuals/AEROX 155 '19 (B65P, B65R, B65S).pdf\"\n",
    "pdf_2= \"Manuals/FJR1300A '15 (1MCH, 1MCG).PDF\"\n",
    "# Format 2: Honda\n",
    "pdf_3 = \"Manuals/CRF1000 A_PC_13MJPG02_(G.H).pdf\"\n",
    "pdf_4 = \"Manuals/NC750XAP_13MKWM02_PC_2022_2023.pdf\"\n",
    "\n",
    "def model_extraction(pdf_path):\n",
    "    base_filename = os.path.basename(pdf_path).split('.')[0]\n",
    "\n",
    "    # Extract model: start of filename, letters/numbers/spaces until a special character (', _)\n",
    "    match = re.match(r\"([A-Za-z0-9 ]+)\", base_filename)\n",
    "    if match:\n",
    "        return match.group(1).replace(\" \", \"\")  # Removes any spaces\n",
    "    \n",
    "    return None\n",
    "\n",
    "def batch_id_extraction(pdf_path, brand):\n",
    "    base_filename = os.path.basename(pdf_path).split('.')[0]\n",
    "\n",
    "    if brand == \"Yamaha\":\n",
    "        # Extract model codes inside parentheses\n",
    "        match = re.search(r\"\\((.*?)\\)\", base_filename)\n",
    "        if match:\n",
    "            parts = match.group(1).split(\",\")\n",
    "            clean_parts = [part.strip() for part in parts]\n",
    "            return \"_\".join(clean_parts)\n",
    "    \n",
    "    elif brand == \"Honda\":\n",
    "        # Look for uppercase/digit code between underscores (6–10 characters)\n",
    "        match = re.search(r\"_([A-Z0-9]{6,10})_\", base_filename)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "    return None\n",
    "\n",
    "def year_extraction(pdf_path, brand):\n",
    "    base_filename = os.path.basename(pdf_path)\n",
    "\n",
    "    if brand == \"Yamaha\":\n",
    "        year_match = re.search(r\"'(\\d{2})\", base_filename)\n",
    "        return f\"20{year_match.group(1)}\" if year_match else None\n",
    "\n",
    "    # elif brand == \"Honda\":\n",
    "    #     # Look for a full year range like 2022_2023\n",
    "    #     match = re.search(r\"(20\\d{2}_20\\d{2})\", base_filename)\n",
    "    #     return match.group(1) if match else None\n",
    "\n",
    "    return None\n",
    "\n",
    "print(batch_id_extraction (pdf_1, \"Yamaha\"))\n",
    "print(batch_id_extraction (pdf_2, \"Yamaha\"))\n",
    "print(batch_id_extraction (pdf_3, \"Honda\"))\n",
    "print(batch_id_extraction (pdf_4, \"Honda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8573d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yamaha + Honda\n",
    "def extract_model(pdf_path):\n",
    "    base_filename = os.path.basename(pdf_path).split('.')[0]\n",
    "\n",
    "    # Extract model: start of filename, letters/numbers/spaces until a special character (', _)\n",
    "    match = re.match(r\"([A-Za-z0-9 ]+)\", base_filename)\n",
    "    if match:\n",
    "        return match.group(1).replace(\" \", \"\")  # Removes any spaces\n",
    "    \n",
    "    return None\n",
    "# Yamaha + Honda\n",
    "def extract_batch_id(pdf_path, brand):\n",
    "    base_filename = os.path.basename(pdf_path).split('.')[0]\n",
    "\n",
    "    if brand == \"Yamaha\":\n",
    "        # Extract model codes inside parentheses\n",
    "        match = re.search(r\"\\((.*?)\\)\", base_filename)\n",
    "        if match:\n",
    "            parts = match.group(1).split(\",\")\n",
    "            clean_parts = [part.strip() for part in parts]\n",
    "            return \"_\".join(clean_parts)\n",
    "    \n",
    "    elif brand == \"Honda\":\n",
    "        # Look for uppercase/digit code between underscores (6–10 characters)\n",
    "        match = re.search(r\"_([A-Z0-9]{6,10})_\", base_filename)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "    return None\n",
    "# Yamaha\n",
    "def extract_year(pdf_path, brand):\n",
    "    base_filename = os.path.basename(pdf_path)\n",
    "\n",
    "    if brand == \"Yamaha\":\n",
    "        year_match = re.search(r\"'(\\d{2})\", base_filename)\n",
    "        return f\"20{year_match.group(1)}\" if year_match else None\n",
    "\n",
    "    # elif brand == \"Honda\":\n",
    "    #     # Look for a full year range like 2022_2023\n",
    "    #     match = re.search(r\"(20\\d{2}_20\\d{2})\", base_filename)\n",
    "    #     return match.group(1) if match else None\n",
    "\n",
    "    return None\n",
    "\n",
    "def reconstruct_lines_from_chars(chars, y_tolerance=2.5):\n",
    "    lines = defaultdict(list)\n",
    "\n",
    "    for c in chars:\n",
    "        # Use midpoint instead of top\n",
    "        y_center = c[\"top\"] + (c[\"height\"] / 2)\n",
    "        y_bucket = round(y_center / y_tolerance)\n",
    "        lines[y_bucket].append(c)\n",
    "\n",
    "    line_texts = []\n",
    "    for y in sorted(lines.keys()):\n",
    "        chars_in_line = sorted(lines[y], key=lambda c: c[\"x0\"])\n",
    "        line = \"\"\n",
    "        prev_x = None\n",
    "\n",
    "        for char in chars_in_line:\n",
    "            x = char[\"x0\"]\n",
    "            text = char[\"text\"]\n",
    "\n",
    "            if prev_x is not None:\n",
    "                gap = x - prev_x\n",
    "                if gap > 1.5:\n",
    "                    line += \" \" * int(gap / 2.5)\n",
    "\n",
    "            line += text\n",
    "            prev_x = char[\"x1\"]\n",
    "\n",
    "        line_texts.append((y, line.rstrip()))\n",
    "\n",
    "    return line_texts\n",
    "\n",
    "def extract_raw_text(pdf_path):\n",
    "    output_lines = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            chars = page.chars\n",
    "            raw_lines = reconstruct_lines_from_chars(chars, y_tolerance=5.5)\n",
    "\n",
    "            # Skip non-parts pages\n",
    "            if not raw_lines or not raw_lines[0][1].strip().startswith(\"FIG.\"):\n",
    "                continue\n",
    "\n",
    "            for _, line in raw_lines:\n",
    "                stripped_line = line.strip()\n",
    "\n",
    "                # ✅ Skip lines that are just a number (likely page numbers)\n",
    "                if re.fullmatch(r\"\\d+\", stripped_line):\n",
    "                    continue\n",
    "\n",
    "                if stripped_line:\n",
    "                    output_lines.append(stripped_line)\n",
    "\n",
    "    return output_lines\n",
    "\n",
    "def structure_raw_text(raw_lines):\n",
    "    structured_output = []\n",
    "    skip_indices = set()\n",
    "\n",
    "    for i in range(len(raw_lines)):\n",
    "        if i in skip_indices:\n",
    "            continue\n",
    "\n",
    "        line = raw_lines[i].strip()\n",
    "        parts = re.split(r\"\\s{2,}\", line)\n",
    "\n",
    "        # --- Normalize FIG. rows to always be ['FIG.', 'number', 'description']\n",
    "        if parts and isinstance(parts[0], str) and parts[0].startswith(\"FIG.\"):\n",
    "            if re.match(r\"^FIG\\.\\s*\\d+$\", parts[0]):\n",
    "                match = re.match(r\"^(FIG\\.)\\s*(\\d+)$\", parts[0])\n",
    "                if match:\n",
    "                    parts = [match.group(1), match.group(2)] + parts[1:]\n",
    "\n",
    "            elif re.match(r\"^FIG\\.\\d+$\", parts[0]):\n",
    "                match = re.match(r\"^(FIG\\.)(\\d+)$\", parts[0])\n",
    "                if match:\n",
    "                    parts = [match.group(1), match.group(2)] + parts[1:]\n",
    "\n",
    "        # --- Skip rows that are just floating descriptions\n",
    "        if len(parts) == 1 and re.match(r\"^[A-Z ,\\-0-9]+$\", parts[0]):\n",
    "            continue\n",
    "\n",
    "        # --- Heuristic: Missing description, try to find it nearby\n",
    "        if len(parts) >= 2 and not re.search(r\"[A-Za-z]\", parts[1]):\n",
    "            # Try backward merge\n",
    "            if i > 0:\n",
    "                prev_line = raw_lines[i - 1].strip()\n",
    "                if len(re.split(r\"\\s{2,}\", prev_line)) == 1:\n",
    "                    parts.insert(1, prev_line)\n",
    "                    skip_indices.add(i - 1)\n",
    "\n",
    "            # Try forward merge\n",
    "            elif i + 1 < len(raw_lines):\n",
    "                next_line = raw_lines[i + 1].strip()\n",
    "                if len(re.split(r\"\\s{2,}\", next_line)) == 1:\n",
    "                    parts.insert(1, next_line)\n",
    "                    skip_indices.add(i + 1)\n",
    "\n",
    "        # --- Extra fix: Split index + part number if mashed into one string\n",
    "        if parts and re.match(r\"^\\d+\\s+[A-Z0-9–\\-]+$\", parts[0]):\n",
    "            split_part = re.split(r\"\\s+\", parts[0], maxsplit=1)\n",
    "            parts = split_part + parts[1:]\n",
    "\n",
    "        structured_output.append(parts)\n",
    "\n",
    "    # --- Final cleanup\n",
    "    structured_output = [\n",
    "        row for row in structured_output\n",
    "        if not (\n",
    "            (len(row) == 1 and re.match(r\"^[A-Z ,\\-0-9]+$\", row[0])) or\n",
    "            all(cell.isdigit() for cell in row)  # <-- remove purely numeric rows\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return structured_output\n",
    "\n",
    "def convert_to_table(pdf_id, year, brand, model, batch_id, structured_output):\n",
    "    rows = []\n",
    "    section = c_name = prev_section = prev_c_name = prev_ref_no = \"\"\n",
    "\n",
    "    for line in structured_output:\n",
    "        if not line or not line[0]:\n",
    "            continue\n",
    "\n",
    "        # FIG. section headers\n",
    "        if line[0] == \"FIG.\" and len(line) >= 3:\n",
    "            section = line[1]\n",
    "\n",
    "            raw_name = \" \".join(line[2:])  # Full raw name with possible number\n",
    "            # Remove trailing digits from component name\n",
    "            c_name = re.sub(r\"\\s*\\d+$\", \"\", raw_name).strip()\n",
    "\n",
    "            prev_section, prev_c_name = section, c_name\n",
    "            continue\n",
    "\n",
    "        # Fallback to previous if not set\n",
    "        if not section:\n",
    "            section, c_name = prev_section, prev_c_name\n",
    "\n",
    "        # Determine if it's a valid data line\n",
    "        if len(line) >= 2 and (re.match(r'\\w+[-–]\\w+', line[0]) or line[0].isdigit()):\n",
    "            if line[0].isdigit():\n",
    "                ref_no = line[0]\n",
    "                part_no = line[1]\n",
    "                rest = line[2:]\n",
    "                prev_ref_no = ref_no\n",
    "            else:\n",
    "                ref_no = prev_ref_no\n",
    "                part_no = line[0]\n",
    "                rest = line[1:]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Extract description and additional info\n",
    "        description = \"\"\n",
    "        add_info = \"\"\n",
    "        numbers = []\n",
    "        found_numbers = False\n",
    "        for item in rest:\n",
    "            if item.isdigit():\n",
    "                numbers.append(item)\n",
    "                found_numbers = True\n",
    "                continue\n",
    "            if not found_numbers:\n",
    "                description += item + \" \"\n",
    "            else:\n",
    "                add_info += item + \" \"\n",
    "\n",
    "        image_id = f\"{pdf_id}_{section}\"\n",
    "\n",
    "        rows.append([\n",
    "            pdf_id, year, brand, model, batch_id, section, c_name,\n",
    "            ref_no, part_no, description.strip(), add_info.strip(), image_id\n",
    "        ])\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        'pdf_id', 'year', 'brand', 'model', 'batch_id',\n",
    "        'section', 'component_name', 'ref_no', 'part_no',\n",
    "        'description', 'add_info', 'image_id'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70664f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format 1: Yamaha\n",
    "pdf_1 = \"Manuals/AEROX 155 '19 (B65P, B65R, B65S).pdf\"\n",
    "pdf_2= \"Manuals/FJR1300A '15 (1MCH, 1MCG).PDF\"\n",
    "pdf_filepath = pdf_2\n",
    "\n",
    "brand = \"Yamaha\"\n",
    "year = extract_year(pdf_filepath, brand)\n",
    "batch_id = extract_batch_id(pdf_filepath, brand)\n",
    "model = extract_model(pdf_filepath)\n",
    "pdf_id = model + '_' + batch_id\n",
    "\n",
    "raw_lines = extract_raw_text(pdf_filepath)\n",
    "structured_data = structure_raw_text(raw_lines)\n",
    "master_parts_list = convert_to_table(pdf_id, year, brand, model, batch_id, structured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f280abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image_background(image_bytes):\n",
    "    img = Image.open(BytesIO(image_bytes)).convert(\"L\")  # Grayscale\n",
    "    mean_brightness = sum(img.getdata()) / (img.width * img.height)\n",
    "\n",
    "    if mean_brightness < 128:  # Invert if it's a dark background\n",
    "        img = ImageOps.invert(img)\n",
    "\n",
    "    img = img.convert(\"RGB\")  # Convert back to RGB\n",
    "\n",
    "    output = BytesIO()\n",
    "    img.save(output, format=\"PNG\")\n",
    "    return output.getvalue()\n",
    "\n",
    "def get_existing_fig_combos(engine, pdf_id):\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(\n",
    "            text(\"SELECT section FROM parts_images WHERE pdf_id = :pdf_id\"),\n",
    "            {\"pdf_id\": pdf_id}\n",
    "        )\n",
    "        return set(str(row[0]) for row in result.fetchall())  # Cast to str for consistent comparison\n",
    "\n",
    "def extract_images_with_fig_labels(pdf_path, pdf_id, engine, brand):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    data = []\n",
    "\n",
    "    seen_figs = set()\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "\n",
    "        # Skip \"Parts Catalogue News\" pages\n",
    "        if \"parts catalogue news\" in text.lower():\n",
    "            print(f\"[Page {page_num+1}] Skipping Parts Catalogue News page\")\n",
    "            continue\n",
    "\n",
    "        if brand == \"Yamaha\":\n",
    "            matches = re.findall(r\"FIG\\.\\s*([\\w-]+)\", text)\n",
    "        elif brand == \"Honda\":\n",
    "            matches = re.findall(r\"\\b((?:E|F|EOP)-\\d{1,3}(?:-\\d+)?)\\b\", text)\n",
    "            print(f\"[Page {page_num+1}] Honda matches: {matches}\")\n",
    "        else:\n",
    "            print(\"Brand not supported\")\n",
    "            return pd.DataFrame(columns=[\"image_id\", \"pdf_id\", \"section\", \"image\"])\n",
    "\n",
    "        # Skip if no section match\n",
    "        if not matches:\n",
    "            continue\n",
    "\n",
    "        # Skip TOC-like pages with multiple sections\n",
    "        if len(matches) > 1:\n",
    "            print(f\"[Page {page_num+1}] Skipping info page with multiple sections number: {matches}\")\n",
    "            continue\n",
    "\n",
    "        section = matches[0]\n",
    "\n",
    "        # Check for required table labels (Honda)\n",
    "        if brand == \"Honda\":\n",
    "            required_patterns = [\n",
    "                r\"ref\\s*\\.?\\s*no\\.?\",\n",
    "                r\"part\\s*no\\.?\",\n",
    "                r\"description\",\n",
    "                r\"reqd\\.?\\s*qty\",\n",
    "                r\"serial\\s*no\\.?\"\n",
    "            ]\n",
    "\n",
    "            text_lower = text.lower()\n",
    "\n",
    "            if not all(re.search(pattern, text_lower, re.IGNORECASE) for pattern in required_patterns):\n",
    "                print(f\"[Page {page_num+1}] Skipping — missing table labels\")\n",
    "                continue\n",
    "\n",
    "        # if section in seen_figs or section in existing_figs:\n",
    "        #     continue\n",
    "\n",
    "        image_list = page.get_images(full=True)\n",
    "        if not image_list:\n",
    "            continue\n",
    "\n",
    "        xref = image_list[0][0]\n",
    "        base_image = doc.extract_image(xref)\n",
    "        image = normalize_image_background(base_image[\"image\"])\n",
    "\n",
    "        image_id = \"_\".join([pdf_id, section])\n",
    "\n",
    "        data.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"pdf_id\": pdf_id,\n",
    "            \"section\": section,\n",
    "            \"image\": image\n",
    "        })\n",
    "        seen_figs.add(section)\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "224f8082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Page 1] Honda matches: []\n",
      "[Page 2] Honda matches: []\n",
      "[Page 3] Honda matches: []\n",
      "[Page 4] Skipping Parts Catalogue News page\n",
      "[Page 5] Honda matches: []\n",
      "[Page 6] Honda matches: []\n",
      "[Page 7] Honda matches: []\n",
      "[Page 8] Honda matches: []\n",
      "[Page 9] Honda matches: ['E-1', 'E-23', 'F-1']\n",
      "[Page 9] Skipping info page with multiple sections number: ['E-1', 'E-23', 'F-1']\n",
      "[Page 10] Honda matches: []\n",
      "[Page 11] Honda matches: []\n",
      "[Page 12] Honda matches: []\n",
      "[Page 13] Honda matches: ['F-39', 'F-39', 'F-24', 'F-44', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46', 'F-46', 'F-46', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50']\n",
      "[Page 13] Skipping info page with multiple sections number: ['F-39', 'F-39', 'F-24', 'F-44', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46', 'F-46', 'F-46', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50']\n",
      "[Page 14] Honda matches: ['F-50', 'F-50', 'F-50', 'F-50', 'F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 14] Skipping info page with multiple sections number: ['F-50', 'F-50', 'F-50', 'F-50', 'F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 15] Honda matches: ['F-39', 'F-39', 'F-24', 'F-44', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46', 'F-46', 'F-46', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50', 'F-45']\n",
      "[Page 15] Skipping info page with multiple sections number: ['F-39', 'F-39', 'F-24', 'F-44', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46', 'F-46', 'F-46', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50', 'F-45']\n",
      "[Page 16] Honda matches: ['F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 16] Skipping info page with multiple sections number: ['F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 17] Honda matches: ['F-39', 'F-24', 'F-44', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46', 'F-46', 'F-46', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50', 'F-45', 'F-45']\n",
      "[Page 17] Skipping info page with multiple sections number: ['F-39', 'F-24', 'F-44', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46', 'F-46', 'F-46', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50', 'F-45', 'F-45']\n",
      "[Page 18] Honda matches: ['F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 18] Skipping info page with multiple sections number: ['F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 19] Honda matches: ['F-39', 'F-24', 'F-44', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46', 'F-46', 'F-46', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50', 'F-45', 'F-45']\n",
      "[Page 19] Skipping info page with multiple sections number: ['F-39', 'F-24', 'F-44', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46', 'F-46', 'F-46', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50', 'F-45', 'F-45']\n",
      "[Page 20] Honda matches: ['F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 20] Skipping info page with multiple sections number: ['F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 21] Honda matches: ['F-39', 'F-39', 'F-24', 'F-44', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46', 'F-46', 'F-46', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50']\n",
      "[Page 21] Skipping info page with multiple sections number: ['F-39', 'F-39', 'F-24', 'F-44', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46', 'F-46', 'F-46', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50']\n",
      "[Page 22] Honda matches: ['F-50', 'F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 22] Skipping info page with multiple sections number: ['F-50', 'F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 23] Honda matches: ['F-39', 'F-39', 'F-24', 'F-44', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46', 'F-46', 'F-46', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50']\n",
      "[Page 23] Skipping info page with multiple sections number: ['F-39', 'F-39', 'F-24', 'F-44', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46', 'F-46', 'F-46', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50']\n",
      "[Page 24] Honda matches: ['F-50', 'F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 24] Skipping info page with multiple sections number: ['F-50', 'F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 25] Honda matches: ['F-39', 'F-39', 'F-24', 'F-44', 'E-6-1', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46-1', 'F-46-1', 'F-46-1', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50']\n",
      "[Page 25] Skipping info page with multiple sections number: ['F-39', 'F-39', 'F-24', 'F-44', 'E-6-1', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46-1', 'F-46-1', 'F-46-1', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50']\n",
      "[Page 26] Honda matches: ['F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 26] Skipping info page with multiple sections number: ['F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 27] Honda matches: ['F-39', 'F-39', 'F-24', 'F-44', 'E-6-1', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46-1', 'F-46-1', 'F-46-1', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50']\n",
      "[Page 27] Skipping info page with multiple sections number: ['F-39', 'F-39', 'F-24', 'F-44', 'E-6-1', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46-1', 'F-46-1', 'F-46-1', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50']\n",
      "[Page 28] Honda matches: ['F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 28] Skipping info page with multiple sections number: ['F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 29] Honda matches: ['F-39', 'F-24', 'F-44', 'E-6-1', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46-1', 'F-46-1', 'F-46-1', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50', 'F-45']\n",
      "[Page 29] Skipping info page with multiple sections number: ['F-39', 'F-24', 'F-44', 'E-6-1', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46-1', 'F-46-1', 'F-46-1', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50', 'F-45']\n",
      "[Page 30] Honda matches: ['F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 30] Skipping info page with multiple sections number: ['F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 31] Honda matches: ['F-39', 'F-24', 'F-44', 'E-6-1', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46-1', 'F-46-1', 'F-46-1', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50', 'F-45']\n",
      "[Page 31] Skipping info page with multiple sections number: ['F-39', 'F-24', 'F-44', 'E-6-1', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46-1', 'F-46-1', 'F-46-1', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50', 'F-50', 'F-45']\n",
      "[Page 32] Honda matches: ['F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 32] Skipping info page with multiple sections number: ['F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 33] Honda matches: ['F-39', 'F-24', 'F-44', 'E-6-1', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46-1', 'F-46-1', 'F-46-1', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50']\n",
      "[Page 33] Skipping info page with multiple sections number: ['F-39', 'F-24', 'F-44', 'E-6-1', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46-1', 'F-46-1', 'F-46-1', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50', 'F-50']\n",
      "[Page 34] Honda matches: ['F-50', 'F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 34] Skipping info page with multiple sections number: ['F-50', 'F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 35] Honda matches: ['F-39', 'F-39', 'F-24', 'F-44', 'E-6-1', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46-1', 'F-46-1', 'F-46-1', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50']\n",
      "[Page 35] Skipping info page with multiple sections number: ['F-39', 'F-39', 'F-24', 'F-44', 'E-6-1', 'F-24', 'F-23-10', 'F-23-10', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-45', 'F-46-1', 'F-46-1', 'F-46-1', 'F-10', 'F-10', 'F-7', 'F-30', 'F-30', 'F-50', 'F-50', 'F-50']\n",
      "[Page 36] Honda matches: ['F-50', 'F-50', 'F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 36] Skipping info page with multiple sections number: ['F-50', 'F-50', 'F-45', 'F-45', 'F-7', 'F-14-40', 'F-23', 'F-23', 'F-44-10', 'F-44-10', 'F-44-10', 'F-44-10', 'F-41', 'F-14-40', 'F-19-40']\n",
      "[Page 37] Honda matches: []\n",
      "[Page 38] Honda matches: []\n",
      "[Page 39] Honda matches: []\n",
      "[Page 40] Honda matches: []\n",
      "[Page 41] Honda matches: []\n",
      "[Page 42] Honda matches: []\n",
      "[Page 43] Honda matches: []\n",
      "[Page 44] Honda matches: []\n",
      "[Page 45] Honda matches: []\n",
      "[Page 46] Honda matches: ['E-1', 'E-2', 'E-3', 'E-4', 'E-6', 'E-6-1', 'E-8', 'E-8-10', 'E-8-20']\n",
      "[Page 46] Skipping info page with multiple sections number: ['E-1', 'E-2', 'E-3', 'E-4', 'E-6', 'E-6-1', 'E-8', 'E-8-10', 'E-8-20']\n",
      "[Page 47] Honda matches: ['E-9', 'E-10', 'E-11', 'E-12', 'E-13', 'E-14', 'E-15', 'E-16', 'E-17']\n",
      "[Page 47] Skipping info page with multiple sections number: ['E-9', 'E-10', 'E-11', 'E-12', 'E-13', 'E-14', 'E-15', 'E-16', 'E-17']\n",
      "[Page 48] Honda matches: ['E-17-10', 'E-17-20', 'E-19', 'E-19-1', 'E-19-10', 'E-21', 'E-23', 'EOP-1', 'EOP-2']\n",
      "[Page 48] Skipping info page with multiple sections number: ['E-17-10', 'E-17-20', 'E-19', 'E-19-1', 'E-19-10', 'E-21', 'E-23', 'EOP-1', 'EOP-2']\n",
      "[Page 49] Honda matches: ['F-1', 'F-2', 'F-4', 'F-4-1', 'F-5-10', 'F-5-20', 'F-5-50', 'F-7', 'F-9']\n",
      "[Page 49] Skipping info page with multiple sections number: ['F-1', 'F-2', 'F-4', 'F-4-1', 'F-5-10', 'F-5-20', 'F-5-50', 'F-7', 'F-9']\n",
      "[Page 50] Honda matches: ['F-10', 'F-11', 'F-12', 'F-14-40', 'F-15', 'F-17', 'F-17-10', 'F-19-40', 'F-21']\n",
      "[Page 50] Skipping info page with multiple sections number: ['F-10', 'F-11', 'F-12', 'F-14-40', 'F-15', 'F-17', 'F-17-10', 'F-19-40', 'F-21']\n",
      "[Page 51] Honda matches: ['F-23', 'F-23-10', 'F-24', 'F-25', 'F-26', 'F-27', 'F-29', 'F-30', 'F-31']\n",
      "[Page 51] Skipping info page with multiple sections number: ['F-23', 'F-23-10', 'F-24', 'F-25', 'F-26', 'F-27', 'F-29', 'F-30', 'F-31']\n",
      "[Page 52] Honda matches: ['F-32', 'F-33', 'F-34', 'F-35', 'F-36', 'F-37', 'F-38', 'F-38-20', 'F-39']\n",
      "[Page 52] Skipping info page with multiple sections number: ['F-32', 'F-33', 'F-34', 'F-35', 'F-36', 'F-37', 'F-38', 'F-38-20', 'F-39']\n",
      "[Page 53] Honda matches: ['F-41', 'F-43', 'F-44', 'F-44-10', 'F-44-20', 'F-45', 'F-46', 'F-46-1', 'F-48']\n",
      "[Page 53] Skipping info page with multiple sections number: ['F-41', 'F-43', 'F-44', 'F-44-10', 'F-44-20', 'F-45', 'F-46', 'F-46-1', 'F-48']\n",
      "[Page 54] Honda matches: ['F-49', 'F-50']\n",
      "[Page 54] Skipping info page with multiple sections number: ['F-49', 'F-50']\n",
      "[Page 55] Honda matches: ['E-1']\n",
      "[Page 56] Honda matches: ['E-2']\n",
      "[Page 57] Honda matches: ['E-2']\n",
      "[Page 57] Skipping — missing table labels\n",
      "[Page 58] Honda matches: ['E-3']\n",
      "[Page 59] Honda matches: ['E-3']\n",
      "[Page 59] Skipping — missing table labels\n",
      "[Page 60] Honda matches: ['E-4']\n",
      "[Page 61] Honda matches: ['E-6']\n",
      "[Page 62] Honda matches: ['E-6-1']\n",
      "[Page 63] Honda matches: ['E-6-1']\n",
      "[Page 64] Honda matches: ['E-8']\n",
      "[Page 65] Honda matches: ['E-8-10']\n",
      "[Page 66] Honda matches: ['E-8-20']\n",
      "[Page 67] Honda matches: ['E-9']\n",
      "[Page 68] Honda matches: ['E-10']\n",
      "[Page 69] Honda matches: ['E-11']\n",
      "[Page 70] Honda matches: ['E-12']\n",
      "[Page 71] Honda matches: ['E-13']\n",
      "[Page 72] Honda matches: ['E-14']\n",
      "[Page 73] Honda matches: ['E-15']\n",
      "[Page 74] Honda matches: ['E-15']\n",
      "[Page 75] Honda matches: ['E-16']\n",
      "[Page 76] Honda matches: ['E-16']\n",
      "[Page 77] Honda matches: ['E-16']\n",
      "[Page 77] Skipping — missing table labels\n",
      "[Page 78] Honda matches: ['E-17']\n",
      "[Page 79] Honda matches: ['E-17']\n",
      "[Page 80] Honda matches: ['E-17-10']\n",
      "[Page 81] Honda matches: ['E-17-20']\n",
      "[Page 82] Honda matches: ['E-19']\n",
      "[Page 83] Honda matches: ['E-19-1']\n",
      "[Page 84] Honda matches: ['E-19-1']\n",
      "[Page 85] Honda matches: ['E-19-10']\n",
      "[Page 86] Honda matches: ['E-21']\n",
      "[Page 87] Honda matches: ['E-23']\n",
      "[Page 88] Honda matches: ['EOP-1']\n",
      "[Page 89] Honda matches: ['EOP-2']\n",
      "[Page 90] Honda matches: ['F-1']\n",
      "[Page 91] Honda matches: ['F-2']\n",
      "[Page 92] Honda matches: ['F-4']\n",
      "[Page 93] Honda matches: ['F-4']\n",
      "[Page 94] Honda matches: ['F-4-1']\n",
      "[Page 95] Honda matches: ['F-5-10']\n",
      "[Page 96] Honda matches: ['F-5-10']\n",
      "[Page 97] Honda matches: ['F-5-20']\n",
      "[Page 98] Honda matches: ['F-5-50']\n",
      "[Page 99] Honda matches: ['F-7']\n",
      "[Page 100] Honda matches: ['F-7']\n",
      "[Page 101] Honda matches: ['F-9']\n",
      "[Page 102] Honda matches: ['F-10']\n",
      "[Page 103] Honda matches: ['F-11']\n",
      "[Page 104] Honda matches: ['F-12']\n",
      "[Page 105] Honda matches: ['F-14-40']\n",
      "[Page 106] Honda matches: ['F-15']\n",
      "[Page 107] Honda matches: ['F-15']\n",
      "[Page 108] Honda matches: ['F-17']\n",
      "[Page 109] Honda matches: ['F-17-10']\n",
      "[Page 110] Honda matches: ['F-19-40']\n",
      "[Page 111] Honda matches: ['F-19-40']\n",
      "[Page 112] Honda matches: ['F-21']\n",
      "[Page 113] Honda matches: ['F-21']\n",
      "[Page 114] Honda matches: ['F-23']\n",
      "[Page 115] Honda matches: ['F-23']\n",
      "[Page 116] Honda matches: ['F-23-10']\n",
      "[Page 117] Honda matches: ['F-24']\n",
      "[Page 118] Honda matches: ['F-25']\n",
      "[Page 119] Honda matches: ['F-26']\n",
      "[Page 120] Honda matches: ['F-27']\n",
      "[Page 121] Honda matches: ['F-27']\n",
      "[Page 122] Honda matches: ['F-29']\n",
      "[Page 123] Honda matches: ['F-30']\n",
      "[Page 124] Honda matches: ['F-31']\n",
      "[Page 125] Honda matches: ['F-32']\n",
      "[Page 126] Honda matches: ['F-32']\n",
      "[Page 127] Honda matches: ['F-33']\n",
      "[Page 128] Honda matches: ['F-34']\n",
      "[Page 129] Honda matches: ['F-35']\n",
      "[Page 130] Honda matches: ['F-36']\n",
      "[Page 131] Honda matches: ['F-36']\n",
      "[Page 132] Honda matches: ['F-37']\n",
      "[Page 133] Honda matches: ['F-38']\n",
      "[Page 134] Honda matches: ['F-38']\n",
      "[Page 135] Honda matches: ['F-38-20']\n",
      "[Page 136] Honda matches: ['F-39']\n",
      "[Page 137] Honda matches: ['F-39']\n",
      "[Page 138] Honda matches: ['F-41']\n",
      "[Page 139] Honda matches: ['F-41']\n",
      "[Page 140] Honda matches: ['F-43']\n",
      "[Page 141] Honda matches: ['F-44']\n",
      "[Page 142] Honda matches: ['F-44-10']\n",
      "[Page 143] Honda matches: ['F-44-10']\n",
      "[Page 144] Honda matches: ['F-44-20']\n",
      "[Page 145] Honda matches: ['F-45']\n",
      "[Page 146] Honda matches: ['F-45']\n",
      "[Page 147] Honda matches: ['F-46']\n",
      "[Page 148] Honda matches: ['F-46-1']\n",
      "[Page 149] Honda matches: ['F-48']\n",
      "[Page 150] Honda matches: ['F-49']\n",
      "[Page 151] Honda matches: ['F-50']\n",
      "[Page 152] Honda matches: []\n",
      "[Page 153] Honda matches: []\n",
      "[Page 154] Honda matches: []\n",
      "[Page 155] Honda matches: []\n",
      "[Page 156] Honda matches: []\n",
      "[Page 157] Honda matches: []\n",
      "[Page 158] Honda matches: []\n",
      "[Page 159] Honda matches: []\n",
      "[Page 160] Honda matches: []\n",
      "[Page 161] Honda matches: []\n",
      "[Page 162] Honda matches: []\n",
      "[Page 163] Honda matches: []\n",
      "[Page 164] Honda matches: []\n",
      "[Page 165] Honda matches: []\n",
      "[Page 166] Honda matches: []\n",
      "[Page 167] Honda matches: []\n",
      "[Page 168] Honda matches: []\n",
      "[Page 169] Skipping Parts Catalogue News page\n",
      "[Page 170] Skipping Parts Catalogue News page\n",
      "[Page 171] Skipping Parts Catalogue News page\n",
      "[Page 172] Skipping Parts Catalogue News page\n",
      "[Page 173] Skipping Parts Catalogue News page\n",
      "[Page 174] Skipping Parts Catalogue News page\n",
      "[Page 175] Skipping Parts Catalogue News page\n",
      "[Page 176] Skipping Parts Catalogue News page\n",
      "[Page 177] Skipping Parts Catalogue News page\n"
     ]
    }
   ],
   "source": [
    "# Format 1: Yamaha\n",
    "pdf_1 = \"Manuals/AEROX 155 '19 (B65P, B65R, B65S).pdf\"\n",
    "pdf_2= \"Manuals/FJR1300A '15 (1MCH, 1MCG).PDF\"\n",
    "# Format 2: Honda\n",
    "pdf_3 = \"Manuals/CRF1000 A_PC_13MJPG02_(G.H).pdf\"\n",
    "pdf_4 = \"Manuals/NC750XAP_13MKWM02_PC_2022_2023.pdf\"\n",
    "\n",
    "pdf_filepath = pdf_4\n",
    "brand = \"Honda\"\n",
    "\n",
    "batch_id = extract_batch_id(pdf_filepath, brand)\n",
    "model = extract_model(pdf_filepath)\n",
    "pdf_id = model + '_' + batch_id\n",
    "\n",
    "image_df = extract_images_with_fig_labels(pdf_filepath, pdf_id, engine, brand)\n",
    "#print(image_df.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef10e1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Page 1] Honda matches: []\n",
      "[Page 2] Skiping Parts Content Page\n",
      "[Page 3] Honda matches: []\n",
      "[Page 4] Skiping Parts Content Page\n",
      "[Page 5] Honda matches: []\n",
      "[Page 6] Honda matches: []\n",
      "[Page 7] Honda matches: []\n",
      "[Page 8] Honda matches: []\n",
      "[Page 9] Honda matches: ['E-1']\n",
      "[Page 9] Skipping — missing table labels\n",
      "[Page 10] Honda matches: []\n",
      "[Page 11] Honda matches: []\n",
      "[Page 12] Honda matches: []\n",
      "[Page 13] Honda matches: []\n",
      "[Page 14] Honda matches: []\n",
      "[Page 15] Honda matches: []\n",
      "[Page 16] Honda matches: []\n",
      "[Page 17] Honda matches: []\n",
      "[Page 18] Honda matches: []\n",
      "[Page 19] Honda matches: []\n",
      "[Page 20] Honda matches: []\n",
      "[Page 21] Honda matches: []\n",
      "[Page 22] Honda matches: []\n",
      "[Page 23] Honda matches: []\n",
      "[Page 24] Honda matches: []\n",
      "[Page 25] Honda matches: []\n",
      "[Page 26] Honda matches: []\n",
      "[Page 27] Honda matches: []\n",
      "[Page 28] Honda matches: []\n",
      "[Page 29] Honda matches: []\n",
      "[Page 30] Honda matches: []\n",
      "[Page 31] Honda matches: []\n",
      "[Page 32] Honda matches: []\n",
      "[Page 33] Honda matches: []\n",
      "[Page 34] Honda matches: []\n",
      "[Page 35] Honda matches: []\n",
      "[Page 36] Honda matches: []\n",
      "[Page 37] Honda matches: []\n",
      "[Page 38] Honda matches: []\n",
      "[Page 39] Honda matches: []\n",
      "[Page 40] Honda matches: []\n",
      "[Page 41] Honda matches: []\n",
      "[Page 42] Honda matches: []\n",
      "[Page 43] Honda matches: []\n",
      "[Page 44] Honda matches: []\n",
      "[Page 45] Honda matches: []\n",
      "[Page 46] Skiping Parts Content Page\n",
      "[Page 47] Skiping Parts Content Page\n",
      "[Page 48] Skiping Parts Content Page\n",
      "[Page 49] Skiping Parts Content Page\n",
      "[Page 50] Skiping Parts Content Page\n",
      "[Page 51] Skiping Parts Content Page\n",
      "[Page 52] Skiping Parts Content Page\n",
      "[Page 53] Skiping Parts Content Page\n",
      "[Page 54] Skiping Parts Content Page\n",
      "[Page 55] Skiping Parts Content Page\n",
      "[Page 56] Skiping Parts Content Page\n",
      "[Page 57] Honda matches: ['E-2']\n",
      "[Page 57] Skipping — missing table labels\n",
      "[Page 58] Skiping Parts Content Page\n",
      "[Page 59] Honda matches: ['E-3']\n",
      "[Page 59] Skipping — missing table labels\n",
      "[Page 60] Skiping Parts Content Page\n",
      "[Page 61] Skiping Parts Content Page\n",
      "[Page 62] Skiping Parts Content Page\n",
      "[Page 63] Honda matches: ['E-6-1']\n",
      "[Page 64] Skiping Parts Content Page\n",
      "[Page 65] Skiping Parts Content Page\n",
      "[Page 66] Skiping Parts Content Page\n",
      "[Page 67] Skiping Parts Content Page\n",
      "[Page 68] Skiping Parts Content Page\n",
      "[Page 69] Skiping Parts Content Page\n",
      "[Page 70] Skiping Parts Content Page\n",
      "[Page 71] Skiping Parts Content Page\n",
      "[Page 72] Skiping Parts Content Page\n",
      "[Page 73] Skiping Parts Content Page\n",
      "[Page 74] Honda matches: ['E-15']\n",
      "[Page 75] Skiping Parts Content Page\n",
      "[Page 76] Honda matches: ['E-16']\n",
      "[Page 77] Honda matches: ['E-16']\n",
      "[Page 77] Skipping — missing table labels\n",
      "[Page 78] Skiping Parts Content Page\n",
      "[Page 79] Honda matches: ['E-17']\n",
      "[Page 80] Skiping Parts Content Page\n",
      "[Page 81] Skiping Parts Content Page\n",
      "[Page 82] Skiping Parts Content Page\n",
      "[Page 83] Skiping Parts Content Page\n",
      "[Page 84] Honda matches: ['E-19-1']\n",
      "[Page 85] Skiping Parts Content Page\n",
      "[Page 86] Skiping Parts Content Page\n",
      "[Page 87] Skiping Parts Content Page\n",
      "[Page 88] Skiping Parts Content Page\n",
      "[Page 89] Skiping Parts Content Page\n",
      "[Page 90] Skiping Parts Content Page\n",
      "[Page 91] Skiping Parts Content Page\n",
      "[Page 92] Skiping Parts Content Page\n",
      "[Page 93] Honda matches: ['F-4']\n",
      "[Page 94] Skiping Parts Content Page\n",
      "[Page 95] Skiping Parts Content Page\n",
      "[Page 96] Honda matches: ['F-5-10']\n",
      "[Page 97] Skiping Parts Content Page\n",
      "[Page 98] Skiping Parts Content Page\n",
      "[Page 99] Skiping Parts Content Page\n",
      "[Page 100] Honda matches: ['F-7']\n",
      "[Page 101] Skiping Parts Content Page\n",
      "[Page 102] Skiping Parts Content Page\n",
      "[Page 103] Skiping Parts Content Page\n",
      "[Page 104] Skiping Parts Content Page\n",
      "[Page 105] Skiping Parts Content Page\n",
      "[Page 106] Skiping Parts Content Page\n",
      "[Page 107] Honda matches: ['F-15']\n",
      "[Page 108] Skiping Parts Content Page\n",
      "[Page 109] Skiping Parts Content Page\n",
      "[Page 110] Skiping Parts Content Page\n",
      "[Page 111] Honda matches: ['F-19-40']\n",
      "[Page 112] Skiping Parts Content Page\n",
      "[Page 113] Honda matches: ['F-21']\n",
      "[Page 114] Skiping Parts Content Page\n",
      "[Page 115] Honda matches: ['F-23']\n",
      "[Page 116] Skiping Parts Content Page\n",
      "[Page 117] Skiping Parts Content Page\n",
      "[Page 118] Skiping Parts Content Page\n",
      "[Page 119] Skiping Parts Content Page\n",
      "[Page 120] Skiping Parts Content Page\n",
      "[Page 121] Honda matches: ['F-27']\n",
      "[Page 122] Skiping Parts Content Page\n",
      "[Page 123] Skiping Parts Content Page\n",
      "[Page 124] Skiping Parts Content Page\n",
      "[Page 125] Skiping Parts Content Page\n",
      "[Page 126] Honda matches: ['F-32']\n",
      "[Page 127] Skiping Parts Content Page\n",
      "[Page 128] Skiping Parts Content Page\n",
      "[Page 129] Skiping Parts Content Page\n",
      "[Page 130] Skiping Parts Content Page\n",
      "[Page 131] Honda matches: ['F-36']\n",
      "[Page 132] Skiping Parts Content Page\n",
      "[Page 133] Skiping Parts Content Page\n",
      "[Page 134] Honda matches: ['F-38']\n",
      "[Page 135] Skiping Parts Content Page\n",
      "[Page 136] Skiping Parts Content Page\n",
      "[Page 137] Honda matches: ['F-39']\n",
      "[Page 138] Skiping Parts Content Page\n",
      "[Page 139] Honda matches: ['F-41']\n",
      "[Page 140] Skiping Parts Content Page\n",
      "[Page 141] Skiping Parts Content Page\n",
      "[Page 142] Skiping Parts Content Page\n",
      "[Page 143] Honda matches: ['F-44-10']\n",
      "[Page 144] Skiping Parts Content Page\n",
      "[Page 145] Skiping Parts Content Page\n",
      "[Page 146] Honda matches: ['F-45']\n",
      "[Page 147] Skiping Parts Content Page\n",
      "[Page 148] Skiping Parts Content Page\n",
      "[Page 149] Skiping Parts Content Page\n",
      "[Page 150] Skiping Parts Content Page\n",
      "[Page 151] Skiping Parts Content Page\n",
      "[Page 152] Honda matches: []\n",
      "[Page 153] Honda matches: []\n",
      "[Page 154] Honda matches: []\n",
      "[Page 155] Honda matches: []\n",
      "[Page 156] Honda matches: []\n",
      "[Page 157] Honda matches: []\n",
      "[Page 158] Honda matches: []\n",
      "[Page 159] Honda matches: []\n",
      "[Page 160] Honda matches: []\n",
      "[Page 161] Honda matches: []\n",
      "[Page 162] Honda matches: []\n",
      "[Page 163] Honda matches: []\n",
      "[Page 164] Honda matches: []\n",
      "[Page 165] Honda matches: []\n",
      "[Page 166] Honda matches: []\n",
      "[Page 167] Skiping Parts Content Page\n",
      "[Page 168] Honda matches: []\n",
      "[Page 169] Skipping Parts Catalogue News page\n",
      "[Page 170] Skipping Parts Catalogue News page\n",
      "[Page 171] Skipping Parts Catalogue News page\n",
      "[Page 172] Skipping Parts Catalogue News page\n",
      "[Page 173] Skipping Parts Catalogue News page\n",
      "[Page 174] Skipping Parts Catalogue News page\n",
      "[Page 175] Skipping Parts Catalogue News page\n",
      "[Page 176] Skipping Parts Catalogue News page\n",
      "[Page 177] Skipping Parts Catalogue News page\n"
     ]
    }
   ],
   "source": [
    "def extract_images_with_fig_labels(pdf_path, pdf_id, engine, brand):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    data = []\n",
    "\n",
    "    seen_figs = set()\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "\n",
    "        # Skip Parts Content pages (multiple images for main group like ENGINE, FRAME GROUP, )\n",
    "        if \"group\" in text.lower():\n",
    "            print(f\"[Page {page_num+1}] Skiping Parts Content Page\")\n",
    "            continue\n",
    "\n",
    "        # Skip \"Parts Catalogue News\" pages\n",
    "        if \"parts catalogue news\" in text.lower():\n",
    "            print(f\"[Page {page_num+1}] Skipping Parts Catalogue News page\")\n",
    "            continue\n",
    "\n",
    "        lines = text.splitlines()\n",
    "        top_lines = \"\\n\".join(lines[:15])  # first 15 lines for section matching\n",
    "\n",
    "        # Match section codes\n",
    "        if brand == \"Yamaha\":\n",
    "            matches = re.findall(r\"FIG\\.\\s*([\\w-]+)\", text)\n",
    "        elif brand == \"Honda\":\n",
    "            matches = re.findall(r\"\\b((?:E|F|EOP)-\\d{1,3}(?:-\\d+)?)\\b\", top_lines)\n",
    "            print(f\"[Page {page_num+1}] Honda matches: {matches}\")\n",
    "        else:\n",
    "            print(\"Brand not supported\")\n",
    "            return pd.DataFrame(columns=[\"image_id\", \"pdf_id\", \"section\", \"image\"])\n",
    "\n",
    "        # Skip if no section match\n",
    "        if not matches:\n",
    "            continue\n",
    "\n",
    "        # Skip TOC-like pages with multiple sections\n",
    "        if len(matches) > 1:\n",
    "            print(f\"[Page {page_num+1}] Skipping info page with multiple sections number: {matches}\")\n",
    "            continue\n",
    "\n",
    "        section = matches[0]\n",
    "\n",
    "        # Check for required table labels (Honda)\n",
    "        if brand == \"Honda\":\n",
    "            required_patterns = [\n",
    "                r\"ref\\s*\\.?\\s*no\\.?\",\n",
    "                r\"part\\s*no\\.?\",\n",
    "                r\"description\",\n",
    "                r\"reqd\\.?\\s*qty\",\n",
    "                r\"serial\\s*no\\.?\"\n",
    "            ]\n",
    "\n",
    "            text_lower = text.lower()\n",
    "\n",
    "            if not all(re.search(pattern, text_lower, re.IGNORECASE) for pattern in required_patterns):\n",
    "                print(f\"[Page {page_num+1}] Skipping — missing table labels\")\n",
    "                continue\n",
    "\n",
    "        # Extract image\n",
    "        image_list = page.get_images(full=True)\n",
    "        if not image_list:\n",
    "            continue\n",
    "\n",
    "        xref = image_list[0][0]\n",
    "        base_image = doc.extract_image(xref)\n",
    "        image = normalize_image_background(base_image[\"image\"])\n",
    "\n",
    "        image_id = \"_\".join([pdf_id, section])\n",
    "\n",
    "        data.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"pdf_id\": pdf_id,\n",
    "            \"section\": section,\n",
    "            \"image\": image\n",
    "        })\n",
    "        seen_figs.add(section)\n",
    "                \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Format 1: Yamaha\n",
    "pdf_1 = \"Manuals/AEROX 155 '19 (B65P, B65R, B65S).pdf\"\n",
    "pdf_2= \"Manuals/FJR1300A '15 (1MCH, 1MCG).PDF\"\n",
    "# Format 2: Honda\n",
    "pdf_3 = \"Manuals/CRF1000 A_PC_13MJPG02_(G.H).pdf\"\n",
    "pdf_4 = \"Manuals/NC750XAP_13MKWM02_PC_2022_2023.pdf\"\n",
    "\n",
    "pdf_filepath = pdf_4\n",
    "brand = \"Honda\"\n",
    "\n",
    "batch_id = extract_batch_id(pdf_filepath, brand)\n",
    "model = extract_model(pdf_filepath)\n",
    "pdf_id = model + '_' + batch_id\n",
    "image_df = extract_images_with_fig_labels(pdf_filepath, pdf_id, engine, brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4690a6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PAGE 138 ===\n",
      "[MAIN GROUP PAGE] → 9 image(s) found\n",
      "Sections found: ['E-1', 'E-2', 'E-3', 'E-4', 'E-5', 'E-5-1', 'E-6', 'E-7', 'E-7-1']\n",
      "\n",
      "=== PAGE 139 ===\n",
      "[MAIN GROUP PAGE] → 9 image(s) found\n",
      "Sections found: ['E-8', 'E-9', 'E-10', 'E-11', 'E-12', 'E-13', 'E-14', 'E-15', 'E-15-1']\n",
      "\n",
      "=== PAGE 140 ===\n",
      "[MAIN GROUP PAGE] → 7 image(s) found\n",
      "Sections found: ['E-15-2', 'E-16', 'E-16-1', 'E-17', 'E-18', 'EOP-1', 'EOP-2']\n",
      "\n",
      "=== PAGE 141 ===\n",
      "[MAIN GROUP PAGE] → 9 image(s) found\n",
      "Sections found: ['F-1', 'F-2', 'F-3', 'F-3-1', 'F-4', 'F-4-1', 'F-5', 'F-6', 'F-7']\n",
      "\n",
      "=== PAGE 142 ===\n",
      "[MAIN GROUP PAGE] → 9 image(s) found\n",
      "Sections found: ['F-8', 'F-9', 'F-10', 'F-11', 'F-12', 'F-13', 'F-13-1', 'F-14', 'F-14-10']\n",
      "\n",
      "=== PAGE 143 ===\n",
      "[MAIN GROUP PAGE] → 9 image(s) found\n",
      "Sections found: ['F-15', 'F-16', 'F-17', 'F-18', 'F-19', 'F-20', 'F-20-10', 'F-21', 'F-22']\n",
      "\n",
      "=== PAGE 144 ===\n",
      "[MAIN GROUP PAGE] → 9 image(s) found\n",
      "Sections found: ['F-23', 'F-24', 'F-25', 'F-26', 'F-27', 'F-28', 'F-29', 'F-29-1', 'F-30']\n",
      "\n",
      "=== PAGE 145 ===\n",
      "[MAIN GROUP PAGE] → 9 image(s) found\n",
      "Sections found: ['F-31', 'F-32', 'F-33', 'F-34', 'F-35', 'F-35-10', 'F-36', 'F-37', 'F-38']\n",
      "\n",
      "=== PAGE 146 ===\n",
      "[MAIN GROUP PAGE] → 4 image(s) found\n",
      "Sections found: ['F-39', 'F-40', 'F-40-1', 'F-40-2']\n",
      "                   image_id            pdf_id section  \\\n",
      "0      Manuals_13MJPG02_E-1  Manuals_13MJPG02     E-1   \n",
      "1      Manuals_13MJPG02_E-2  Manuals_13MJPG02     E-2   \n",
      "2      Manuals_13MJPG02_E-3  Manuals_13MJPG02     E-3   \n",
      "3      Manuals_13MJPG02_E-4  Manuals_13MJPG02     E-4   \n",
      "4      Manuals_13MJPG02_E-5  Manuals_13MJPG02     E-5   \n",
      "..                      ...               ...     ...   \n",
      "69    Manuals_13MJPG02_F-38  Manuals_13MJPG02    F-38   \n",
      "70    Manuals_13MJPG02_F-39  Manuals_13MJPG02    F-39   \n",
      "71    Manuals_13MJPG02_F-40  Manuals_13MJPG02    F-40   \n",
      "72  Manuals_13MJPG02_F-40-1  Manuals_13MJPG02  F-40-1   \n",
      "73  Manuals_13MJPG02_F-40-2  Manuals_13MJPG02  F-40-2   \n",
      "\n",
      "                                                image  \n",
      "0   b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  \n",
      "1   b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  \n",
      "2   b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  \n",
      "3   b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  \n",
      "4   b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  \n",
      "..                                                ...  \n",
      "69  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  \n",
      "70  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  \n",
      "71  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  \n",
      "72  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  \n",
      "73  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...  \n",
      "\n",
      "[74 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "def honda_extract_images_with_fig_labels(pdf_path, pdf_id):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    data = []\n",
    "\n",
    "    MAIN_GROUPS = [\"ENGINEGROUP\", \"FRAMEGROUP\"]\n",
    "\n",
    "    section_pattern = r\"\\b((?:E|F|EOP)-\\d{1,3}(?:-\\d+)?)\\b\"\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        lines = text.splitlines()\n",
    "\n",
    "        # --- Check if page is a MAIN GROUP page ---\n",
    "        page_has_main_group = False\n",
    "\n",
    "        text_no_spaces = re.sub(r\"\\s+\", \"\", text).lower()\n",
    "\n",
    "        for group in MAIN_GROUPS:\n",
    "            if group.lower() in text_no_spaces:\n",
    "                page_has_main_group = True\n",
    "                break\n",
    "\n",
    "        if not page_has_main_group:\n",
    "            continue  # skip page\n",
    "\n",
    "        # --- Check if page has images ---\n",
    "        image_list = page.get_images()\n",
    "        if not image_list:\n",
    "            continue  # skip if no images\n",
    "\n",
    "        # --- Extract section labels from page ---\n",
    "        sections_found = []\n",
    "        for line in lines:\n",
    "            match = re.search(section_pattern, line)\n",
    "            if match:\n",
    "                section = match.group(1)\n",
    "                sections_found.append(section)\n",
    "\n",
    "        if not sections_found:\n",
    "            print(f\"\\n=== PAGE {page_num+1} ===\")\n",
    "            print(\"[SKIP] No sections found\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== PAGE {page_num+1} ===\")\n",
    "        print(f\"[MAIN GROUP PAGE] → {len(image_list)} image(s) found\")\n",
    "        print(f\"Sections found: {sections_found}\")\n",
    "\n",
    "        # --- Map each section to corresponding image ---\n",
    "        # NOTE: assumes order of section labels = order of images\n",
    "        for idx, section in enumerate(sections_found):\n",
    "            if idx >= len(image_list):\n",
    "                print(f\"⚠️ Not enough images for sections — stopping at {idx}\")\n",
    "                break\n",
    "\n",
    "            image_info = image_list[idx]\n",
    "            xref = image_info[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image = normalize_image_background(base_image[\"image\"])\n",
    "\n",
    "            image_id = f\"{pdf_id}_{section}\"\n",
    "\n",
    "            data.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"pdf_id\": pdf_id,\n",
    "                \"section\": section,\n",
    "                \"image\": image\n",
    "            })\n",
    "\n",
    "            # # For debug: display the section + image\n",
    "            # img = Image.open(BytesIO(image))\n",
    "            # display(img)\n",
    "\n",
    "            #print(f\"[PAGE {page_num+1}] {section} → Image saved\")\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Format 1: Yamaha\n",
    "pdf_1 = \"Manuals/AEROX 155 '19 (B65P, B65R, B65S).pdf\"\n",
    "pdf_2= \"Manuals/FJR1300A '15 (1MCH, 1MCG).PDF\"\n",
    "# Format 2: Honda\n",
    "pdf_3 = \"Manuals/CRF1000 A_PC_13MJPG02_(G.H).pdf\"\n",
    "pdf_4 = \"Manuals/NC750XAP_13MKWM02_PC_2022_2023.pdf\"\n",
    "\n",
    "pdf_filepath = pdf_3\n",
    "brand = \"Honda\"\n",
    "\n",
    "batch_id = extract_batch_id(pdf_filepath, brand)\n",
    "model = extract_model(pdf_filepath)\n",
    "pdf_id = model + '_' + batch_id\n",
    "image_df = honda_extract_images_with_fig_labels(pdf_filepath, pdf_id)\n",
    "print (image_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0651f93",
   "metadata": {},
   "source": [
    "### Object Creation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b249e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFProcessor:\n",
    "    def __init__(self, pdf_path, pdf_id, brand, model, batch_id, year):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.pdf_id = pdf_id\n",
    "        self.brand = brand\n",
    "        self.model = model\n",
    "        self.batch_id = batch_id\n",
    "        self.year = year\n",
    "        \n",
    "    @staticmethod\n",
    "    def normalize_image_background(image_bytes):\n",
    "        img = Image.open(BytesIO(image_bytes)).convert(\"L\")  # Grayscale\n",
    "        mean_brightness = sum(img.getdata()) / (img.width * img.height)\n",
    "        if mean_brightness < 128:\n",
    "            img = ImageOps.invert(img)\n",
    "        img = img.convert(\"RGB\")\n",
    "        output = BytesIO()\n",
    "        img.save(output, format=\"PNG\")\n",
    "        return output.getvalue()\n",
    "\n",
    "    def extract_text(self):\n",
    "        raise NotImplementedError(\"Each brand must implement its own text extraction\")\n",
    "\n",
    "    def extract_images(self, engine):\n",
    "        raise NotImplementedError(\"Each brand must implement its own image extraction\")\n",
    "\n",
    "class YamahaProcessor(PDFProcessor):\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruct_lines_from_chars(chars, y_tolerance=2.5):\n",
    "        lines = defaultdict(list)\n",
    "        for c in chars:\n",
    "            y_center = c[\"top\"] + (c[\"height\"] / 2)\n",
    "            y_bucket = round(y_center / y_tolerance)\n",
    "            lines[y_bucket].append(c)\n",
    "        line_texts = []\n",
    "        for y in sorted(lines.keys()):\n",
    "            chars_in_line = sorted(lines[y], key=lambda c: c[\"x0\"])\n",
    "            line = \"\"\n",
    "            prev_x = None\n",
    "            for char in chars_in_line:\n",
    "                x = char[\"x0\"]\n",
    "                text = char[\"text\"]\n",
    "                if prev_x is not None:\n",
    "                    gap = x - prev_x\n",
    "                    if gap > 1.5:\n",
    "                        line += \" \" * int(gap / 2.5)\n",
    "                line += text\n",
    "                prev_x = char[\"x1\"]\n",
    "            line_texts.append((y, line.rstrip()))\n",
    "        return line_texts\n",
    "\n",
    "    def extract_raw_text(self, pdf_path):\n",
    "        output_lines = []\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                chars = page.chars\n",
    "                raw_lines = self.reconstruct_lines_from_chars(chars, y_tolerance=5.5)\n",
    "                if not raw_lines or not raw_lines[0][1].strip().startswith(\"FIG.\"):\n",
    "                    continue\n",
    "                for _, line in raw_lines:\n",
    "                    stripped_line = line.strip()\n",
    "                    if re.fullmatch(r\"\\d+\", stripped_line):\n",
    "                        continue\n",
    "                    if stripped_line:\n",
    "                        output_lines.append(stripped_line)\n",
    "        return output_lines\n",
    "\n",
    "    @staticmethod\n",
    "    def structure_raw_text(raw_lines):\n",
    "        structured_output = []\n",
    "        skip_indices = set()\n",
    "\n",
    "        for i in range(len(raw_lines)):\n",
    "            if i in skip_indices:\n",
    "                continue\n",
    "\n",
    "            line = raw_lines[i].strip()\n",
    "            parts = re.split(r\"\\s{2,}\", line)\n",
    "\n",
    "            # --- Normalize FIG. rows to always be ['FIG.', 'number', 'description']\n",
    "            if parts and isinstance(parts[0], str) and parts[0].startswith(\"FIG.\"):\n",
    "                if re.match(r\"^FIG\\.\\s*\\d+$\", parts[0]):\n",
    "                    match = re.match(r\"^(FIG\\.)\\s*(\\d+)$\", parts[0])\n",
    "                    if match:\n",
    "                        parts = [match.group(1), match.group(2)] + parts[1:]\n",
    "\n",
    "                elif re.match(r\"^FIG\\.\\d+$\", parts[0]):\n",
    "                    match = re.match(r\"^(FIG\\.)(\\d+)$\", parts[0])\n",
    "                    if match:\n",
    "                        parts = [match.group(1), match.group(2)] + parts[1:]\n",
    "\n",
    "            # --- Skip rows that are just floating descriptions\n",
    "            if len(parts) == 1 and re.match(r\"^[A-Z ,\\-0-9]+$\", parts[0]):\n",
    "                continue\n",
    "\n",
    "            # --- Heuristic: Missing description, try to find it nearby\n",
    "            if len(parts) >= 2 and not re.search(r\"[A-Za-z]\", parts[1]):\n",
    "                # Try backward merge\n",
    "                if i > 0:\n",
    "                    prev_line = raw_lines[i - 1].strip()\n",
    "                    if len(re.split(r\"\\s{2,}\", prev_line)) == 1:\n",
    "                        parts.insert(1, prev_line)\n",
    "                        skip_indices.add(i - 1)\n",
    "\n",
    "                # Try forward merge\n",
    "                elif i + 1 < len(raw_lines):\n",
    "                    next_line = raw_lines[i + 1].strip()\n",
    "                    if len(re.split(r\"\\s{2,}\", next_line)) == 1:\n",
    "                        parts.insert(1, next_line)\n",
    "                        skip_indices.add(i + 1)\n",
    "\n",
    "            # --- Extra fix: Split index + part number if mashed into one string\n",
    "            if parts and re.match(r\"^\\d+\\s+[A-Z0-9–\\-]+$\", parts[0]):\n",
    "                split_part = re.split(r\"\\s+\", parts[0], maxsplit=1)\n",
    "                parts = split_part + parts[1:]\n",
    "\n",
    "            structured_output.append(parts)\n",
    "\n",
    "        # --- Final cleanup\n",
    "        structured_output = [\n",
    "            row for row in structured_output\n",
    "            if not (\n",
    "                (len(row) == 1 and re.match(r\"^[A-Z ,\\-0-9]+$\", row[0])) or\n",
    "                all(cell.isdigit() for cell in row)  # <-- remove purely numeric rows\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return structured_output\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_table(pdf_id, year, brand, model, batch_id, structured_output):\n",
    "        rows = []\n",
    "        section = s_name = prev_section = prev_c_name = prev_ref_no = \"\"\n",
    "\n",
    "        for line in structured_output:\n",
    "            if not line or not line[0]:\n",
    "                continue\n",
    "\n",
    "            # FIG. section headers\n",
    "            if line[0] == \"FIG.\" and len(line) >= 3:\n",
    "                section = line[1]\n",
    "\n",
    "                raw_name = \" \".join(line[2:])  # Full raw name with possible number\n",
    "                # Remove trailing digits from component name\n",
    "                s_name = re.sub(r\"\\s*\\d+$\", \"\", raw_name).strip()\n",
    "\n",
    "                prev_section, prev_c_name = section, s_name\n",
    "                continue\n",
    "\n",
    "            # Fallback to previous if not set\n",
    "            if not section:\n",
    "                section, s_name = prev_section, prev_c_name\n",
    "\n",
    "            # Determine if it's a valid data line\n",
    "            if len(line) >= 2 and (re.match(r'\\w+[-–]\\w+', line[0]) or line[0].isdigit()):\n",
    "                if line[0].isdigit():\n",
    "                    ref_no = line[0]\n",
    "                    part_no = line[1]\n",
    "                    rest = line[2:]\n",
    "                    prev_ref_no = ref_no\n",
    "                else:\n",
    "                    ref_no = prev_ref_no\n",
    "                    part_no = line[0]\n",
    "                    rest = line[1:]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Extract description and additional info\n",
    "            description = \"\"\n",
    "            remarks = \"\"\n",
    "            numbers = []\n",
    "            found_numbers = False\n",
    "            for item in rest:\n",
    "                if item.isdigit():\n",
    "                    numbers.append(item)\n",
    "                    found_numbers = True\n",
    "                    continue\n",
    "                if not found_numbers:\n",
    "                    description += item + \" \"\n",
    "                else:\n",
    "                    remarks += item + \" \"\n",
    "\n",
    "            image_id = f\"{pdf_id}_{section}\"\n",
    "\n",
    "            rows.append([\n",
    "                pdf_id, year, brand, model, batch_id, section, s_name,\n",
    "                ref_no, part_no, description.strip(), remarks.strip(), image_id\n",
    "            ])\n",
    "\n",
    "        return pd.DataFrame(rows, columns=[\n",
    "            'pdf_id', 'year', 'brand', 'model', 'batch_id',\n",
    "            'section', 'section_name', 'ref_no', 'part_no',\n",
    "            'description', 'remarks', 'image_id'\n",
    "        ])\n",
    "\n",
    "    def yamaha_extract_images_with_fig_labels(self, pdf_stream, pdf_id):\n",
    "        doc = fitz.open(stream=pdf_stream, filetype=\"pdf\")\n",
    "        data = []\n",
    "        seen_figs = set()\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text = page.get_text()\n",
    "            matches = re.findall(r\"FIG\\.\\s*([\\w-]+)\", text)\n",
    "            if not matches:\n",
    "                continue\n",
    "            section = matches[0]\n",
    "            if section in seen_figs:\n",
    "                continue\n",
    "            image_list = page.get_images(full=True)\n",
    "            if not image_list:\n",
    "                continue\n",
    "            xref = image_list[0][0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image = self.normalize_image_background(base_image[\"image\"])\n",
    "            image_id = f\"{pdf_id}_{section}\"\n",
    "            data.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"pdf_id\": pdf_id,\n",
    "                \"section\": section,\n",
    "                \"image\": image\n",
    "            })\n",
    "            seen_figs.add(section)\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def extract_text(self):\n",
    "        raw_lines = self.extract_raw_text(self.pdf_path)\n",
    "        structured_data = self.structure_raw_text(raw_lines)\n",
    "        df = self.convert_to_table(\n",
    "            pdf_id=self.pdf_id,\n",
    "            year=self.year,\n",
    "            brand=self.brand,\n",
    "            model=self.model,\n",
    "            batch_id=self.batch_id,\n",
    "            structured_output=structured_data\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def extract_images(self):\n",
    "        with open(self.pdf_path, \"rb\") as f:\n",
    "            pdf_stream = f.read()\n",
    "        df = self.yamaha_extract_images_with_fig_labels(\n",
    "            pdf_stream=pdf_stream,\n",
    "            pdf_id=self.pdf_id\n",
    "        )\n",
    "        return df\n",
    "\n",
    "class HondaProcessor(PDFProcessor):\n",
    "    @staticmethod\n",
    "    def extract_section_with_layout(pdf_path: str, section_code: str, section_title: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Finds a specified section, locates 'Reqd. QTY', extracts in layout mode,\n",
    "        then parses each part and variant into ref_no, part_no, description, remarks.\n",
    "        Stops collecting once it encounters any line containing 'PART', 'NO', and 'INDEX'.\n",
    "        Returns a DataFrame with columns ref_no, part_no, description, remarks.\n",
    "        \"\"\"\n",
    "        code = section_code.upper()\n",
    "        title = section_title.upper()\n",
    "\n",
    "        next_sec_re     = re.compile(r'^[A-Z]+-\\d+', re.IGNORECASE)\n",
    "        table_header_re = re.compile(r'\\bReqd\\.?\\s*QTY\\b', re.IGNORECASE)\n",
    "        part_no_re      = re.compile(r'\\b[0-9]{5,}(?:-[A-Z0-9-]+)+\\b')\n",
    "        end_re          = re.compile(r'.*PART\\s*NO\\.?\\s*INDEX.*', re.IGNORECASE)\n",
    "\n",
    "        # Phase 1: locate page range\n",
    "        start_page = header_hit = None\n",
    "        end_page = None\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                for ln in (page.extract_text() or \"\").splitlines():\n",
    "                    u = ln.strip().upper()\n",
    "                    if start_page is None:\n",
    "                        if ((\"FRAMEGROUP\" in u and u.startswith(code) and title in u)\n",
    "                            or (u.startswith(code) and title in u)):\n",
    "                            start_page = i\n",
    "                            break\n",
    "                    elif not header_hit:\n",
    "                        if table_header_re.search(u):\n",
    "                            header_hit = True\n",
    "                    else:\n",
    "                        if next_sec_re.match(u) and not u.startswith(code):\n",
    "                            end_page = i\n",
    "                            break\n",
    "                if end_page is not None:\n",
    "                    break\n",
    "            if start_page is None or not header_hit:\n",
    "                raise ValueError(f\"Section '{section_code} {section_title}' not found or missing table header.\")\n",
    "            if end_page is None:\n",
    "                end_page = len(pdf.pages)\n",
    "\n",
    "            # Phase 2: collect layout-preserved lines\n",
    "            collected = []\n",
    "            in_table = False\n",
    "            stop_all = False\n",
    "            for pi in range(start_page, end_page):\n",
    "                for ln in (pdf.pages[pi].extract_text(layout=True) or \"\").splitlines():\n",
    "                    u = ln.strip().upper()\n",
    "                    if end_re.match(u):\n",
    "                        stop_all = True\n",
    "                        break\n",
    "                    if not in_table:\n",
    "                        if table_header_re.search(u):\n",
    "                            in_table = True\n",
    "                        continue\n",
    "                    if next_sec_re.match(u) and not u.startswith(code):\n",
    "                        break\n",
    "                    collected.append(ln)\n",
    "                if stop_all:\n",
    "                    break\n",
    "\n",
    "        # Phase 3: group into per-part buffers\n",
    "        records = []\n",
    "        last_ref = \"\"\n",
    "        for ln in collected:\n",
    "            m_pno = part_no_re.search(ln)\n",
    "            if m_pno:\n",
    "                m_ref = re.match(r'^\\s*(?:\\((\\d+)\\)|(\\d+))\\s+', ln)\n",
    "                if m_ref:\n",
    "                    last_ref = m_ref.group(1) or m_ref.group(2)\n",
    "                records.append({\n",
    "                    \"ref\":      last_ref,\n",
    "                    \"part_no\":  m_pno.group(0),\n",
    "                    \"buf\":      [ln[m_pno.end():].strip()]\n",
    "                })\n",
    "            else:\n",
    "                if not records:\n",
    "                    continue\n",
    "                txt = ln.strip()\n",
    "                if re.fullmatch(r'\\d+', txt) or re.fullmatch(r'\\d{4}\\.\\d{2}\\.\\d{2}', txt):\n",
    "                    continue\n",
    "                records[-1][\"buf\"].append(txt)\n",
    "\n",
    "        # Phase 4: parse each buffer directly into column-lists\n",
    "        ref_nos      = []\n",
    "        part_nos     = []\n",
    "        descriptions = []\n",
    "        remarks_list = []\n",
    "\n",
    "        for rec in records:\n",
    "            raw = \" \".join(rec[\"buf\"])\n",
    "            raw = raw.replace('∙','').replace('•','').replace('\\uf020','')\n",
    "            raw = re.sub(r'\\s+', ' ', raw).strip()\n",
    "\n",
    "            idx       = raw.find(\"--------\")\n",
    "            desc_part = raw[:idx].strip() if idx != -1 else raw\n",
    "            cat_part  = raw[idx+8:].strip() if idx != -1 else \"\"\n",
    "\n",
    "            # clean up description\n",
    "            desc_part = re.sub(r'\\.{2,}\\s+\\d.*$', '', desc_part).strip()\n",
    "            desc_part = re.sub(r'\\s+GK[A-Za-z0-9]+\\s*$', '', desc_part)\n",
    "            desc_part = re.sub(r'\\s+(?:-+|\\d+)+\\s*$', '', desc_part)\n",
    "            desc      = re.sub(r'\\s+\\d+\\s+\\d{4}\\.\\d{2}\\.\\d{2}.*$', \"\", desc_part).strip()\n",
    "            desc      = re.sub(r'(?:\\s+(?:\\(\\d+\\)|-+|\\d+))+$',     \"\", desc).strip()\n",
    "            desc      = re.sub(r'\\.{2,}$',                         \"\", desc).strip()\n",
    "            desc      = re.sub(r'(?:\\s+[A-Z])+$',                  \"\", desc).strip()\n",
    "            desc      = \"\" if not re.search(r'[A-Za-z]', desc) else desc\n",
    "\n",
    "            # clean up catalogue codes → remarks\n",
    "            if cat_part.upper().startswith(\"GK\") and len(cat_part) > 8:\n",
    "                cat_clean = cat_part[8:].split()[0]\n",
    "            else:\n",
    "                m_codes   = re.match(r'[-\\s]*([0-9A-Z,\\s]+)', cat_part)\n",
    "                raw_codes = m_codes.group(1) if m_codes else \"\"\n",
    "                cat_clean = raw_codes.replace(\" \", \"\")\n",
    "                cat_clean = re.sub(r'([A-Z])(?=\\d)', r'\\1,', cat_clean)\n",
    "                cat_clean = re.sub(r'(?<=[0-9A-Z]{2})(?=[A-Z]{2}(?:,|$))', ',', cat_clean)\n",
    "            cat_clean    = re.sub(r'\\d{4}$', '', cat_clean)\n",
    "            tokens       = [t for t in cat_clean.split(',') if t]\n",
    "            seen         = set()\n",
    "            final_codes  = [c for c in tokens if c not in seen and not seen.add(c)]\n",
    "            remarks      = \",\".join(final_codes)\n",
    "\n",
    "            # adjust part_no suffix logic\n",
    "            m3 = re.match(r'^(.+?)([A-Z]{3,})$', rec[\"part_no\"])\n",
    "            if m3:\n",
    "                core, suf = m3.group(1), m3.group(2)\n",
    "                part_no    = core + suf[:2]\n",
    "                desc       = f\"{suf[2:]} {desc}\".strip()\n",
    "            else:\n",
    "                part_no = rec[\"part_no\"]\n",
    "\n",
    "            ref_nos.append(rec[\"ref\"])\n",
    "            part_nos.append(part_no)\n",
    "            descriptions.append(desc)\n",
    "            remarks_list.append(remarks)\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'ref_no':      ref_nos,\n",
    "            'part_no':     part_nos,\n",
    "            'description': descriptions,\n",
    "            'remarks':     remarks_list\n",
    "        })\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_all_sections_one_pass(pdf_id, year, brand, model, batch_id, pdf_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Opens the PDF once, walks through it page by page, detects sections using\n",
    "        next_sec_re, collects each section’s lines, inlines Phase 3+4 verbatim,\n",
    "        stops entirely when end_re is first encountered, strips any leading\n",
    "        \"*GROUP\" from titles, and writes a CSV with columns\n",
    "        section_no, section_name, ref_no, part_no, description, remarks.\n",
    "        \"\"\"\n",
    "        next_sec_re     = re.compile(r'^[A-Z]+-\\d+', re.IGNORECASE)\n",
    "        table_header_re = re.compile(r'\\bReqd\\.?\\s*QTY\\b', re.IGNORECASE)\n",
    "        part_no_re      = re.compile(r'\\b[0-9]{5,}(?:-[A-Z0-9-]+)+\\b')\n",
    "        end_re          = re.compile(r'.*PART\\s*NO\\.?\\s*INDEX.*', re.IGNORECASE)\n",
    "\n",
    "        section_nos   = []\n",
    "        section_names = []\n",
    "        ref_nos       = []\n",
    "        part_nos      = []\n",
    "        descriptions  = []\n",
    "        remarks_list  = []\n",
    "\n",
    "        current = None\n",
    "        done    = False\n",
    "\n",
    "        def _flush(cur):\n",
    "            \"\"\"Phase 3+4 logic verbatim, flushing cur['collected'] into our lists.\"\"\"\n",
    "            records = []; last_ref = \"\"\n",
    "            for ln in cur['collected']:\n",
    "                m_pno = part_no_re.search(ln)\n",
    "                if m_pno:\n",
    "                    m_ref = re.match(r'^\\s*(?:\\((\\d+)\\)|(\\d+))\\s+', ln)\n",
    "                    if m_ref:\n",
    "                        last_ref = m_ref.group(1) or m_ref.group(2)\n",
    "                    records.append({\n",
    "                        'ref': last_ref,\n",
    "                        'part_no': m_pno.group(0),\n",
    "                        'buf': [ln[m_pno.end():].strip()]\n",
    "                    })\n",
    "                else:\n",
    "                    if not records: continue\n",
    "                    txt = ln.strip()\n",
    "                    if re.fullmatch(r'\\d+', txt) or re.fullmatch(r'\\d{4}\\.\\d{2}\\.\\d{2}', txt):\n",
    "                        continue\n",
    "                    records[-1]['buf'].append(txt)\n",
    "\n",
    "            for rec in records:\n",
    "                raw = \" \".join(rec['buf']).replace('∙','').replace('•','').replace('\\uf020','')\n",
    "                raw = re.sub(r'\\s+', ' ', raw).strip()\n",
    "                idx = raw.find(\"--------\")\n",
    "                desc_part = raw[:idx].strip() if idx != -1 else raw\n",
    "                cat_part  = raw[idx+8:].strip() if idx != -1 else \"\"\n",
    "\n",
    "                # description cleanup\n",
    "                desc_part = re.sub(r'\\.{2,}\\s+\\d.*$', '', desc_part).strip()\n",
    "                desc_part = re.sub(r'\\s+GK[A-Za-z0-9]+\\s*$', '', desc_part)\n",
    "                desc_part = re.sub(r'\\s+(?:-+|\\d+)+\\s*$', '', desc_part)\n",
    "                desc = re.sub(r'\\s+\\d+\\s+\\d{4}\\.\\d{2}\\.\\d{2}.*$', \"\", desc_part).strip()\n",
    "                desc = re.sub(r'(?:\\s+(?:\\(\\d+\\)|-+|\\d+))+$', \"\", desc).strip()\n",
    "                desc = re.sub(r'\\.{2,}$', \"\", desc).strip()\n",
    "                desc = re.sub(r'(?:\\s+[A-Z])+$', \"\", desc).strip()\n",
    "                desc = \"\" if not re.search(r'[A-Za-z]', desc) else desc\n",
    "\n",
    "                # remarks cleanup\n",
    "                if cat_part.upper().startswith(\"GK\") and len(cat_part) > 8:\n",
    "                    cat_clean = cat_part[8:].split()[0]\n",
    "                else:\n",
    "                    m_codes   = re.match(r'[-\\s]*([0-9A-Z,\\s]+)', cat_part)\n",
    "                    raw_codes = m_codes.group(1) if m_codes else \"\"\n",
    "                    cat_clean = raw_codes.replace(\" \", \"\")\n",
    "                    cat_clean = re.sub(r'([A-Z])(?=\\d)', r'\\1,', cat_clean)\n",
    "                    cat_clean = re.sub(r'(?<=[0-9A-Z]{2})(?=[A-Z]{2}(?:,|$))', ',', cat_clean)\n",
    "                cat_clean   = re.sub(r'\\d{4}$', '', cat_clean)\n",
    "                tokens      = [t for t in cat_clean.split(',') if t]\n",
    "                seen        = set()\n",
    "                final_codes = [c for c in tokens if c not in seen and not seen.add(c)]\n",
    "                remarks     = \",\".join(final_codes)\n",
    "\n",
    "                m3 = re.match(r'^(.+?)([A-Z]{3,})$', rec['part_no'])\n",
    "                if m3:\n",
    "                    core, suf = m3.group(1), m3.group(2)\n",
    "                    pno        = core + suf[:2]\n",
    "                    desc       = f\"{suf[2:]} {desc}\".strip()\n",
    "                else:\n",
    "                    pno = rec['part_no']\n",
    "\n",
    "                section_nos.append(cur['code'])\n",
    "                section_names.append(cur['title'])\n",
    "                ref_nos.append(rec['ref'])\n",
    "                part_nos.append(pno)\n",
    "                descriptions.append(desc)\n",
    "                remarks_list.append(remarks)\n",
    "\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                plain  = (page.extract_text() or \"\").splitlines()\n",
    "                layout = (page.extract_text(layout=True) or \"\").splitlines()\n",
    "\n",
    "                # detect new section headers\n",
    "                for ln in plain:\n",
    "                    if done:\n",
    "                        break\n",
    "                    u = ln.strip().upper()\n",
    "                    if next_sec_re.match(u):\n",
    "                        if current:\n",
    "                            _flush(current)\n",
    "                        parts = ln.strip().split(None, 1)\n",
    "                        raw_title = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "                        # strip any leading \"*GROUP\"\n",
    "                        title = re.sub(r'\\b[A-Z]+GROUP\\b\\s*', '', raw_title, flags=re.IGNORECASE)\n",
    "                        current = {\n",
    "                            'code':       parts[0].upper(),\n",
    "                            'title':      title,\n",
    "                            'header_hit': False,\n",
    "                            'collected':  []\n",
    "                        }\n",
    "\n",
    "                # collect layout lines\n",
    "                if current:\n",
    "                    for ln in layout:\n",
    "                        u = ln.strip().upper()\n",
    "                        if end_re.match(u):\n",
    "                            _flush(current)\n",
    "                            done = True\n",
    "                            break\n",
    "                        if not current['header_hit']:\n",
    "                            if table_header_re.search(u):\n",
    "                                current['header_hit'] = True\n",
    "                            continue\n",
    "                        if next_sec_re.match(u) and not u.startswith(current['code']):\n",
    "                            _flush(current)\n",
    "                            current = None\n",
    "                            break\n",
    "                        current['collected'].append(ln)\n",
    "\n",
    "        if current and not done:\n",
    "            _flush(current)\n",
    "\n",
    "        final_df = pd.DataFrame({\n",
    "            'pdf_id': pdf_id,       #added\n",
    "            'year': year,           #added\n",
    "            'brand': brand,         #added\n",
    "            'model': model,         #added\n",
    "            'batch_id': batch_id,   #added\n",
    "            'section':   section_nos,\n",
    "            'section_name': section_names,\n",
    "            'ref_no':       ref_nos,\n",
    "            'part_no':      part_nos,\n",
    "            'description':  descriptions,\n",
    "            'remarks':      remarks_list,\n",
    "        })\n",
    "        final_df[\"image_id\"] = final_df[\"pdf_id\"] + \"_\" + final_df[\"section\"]\n",
    "        return final_df\n",
    "    \n",
    "    def honda_extract_images_with_fig_labels(pdf_stream, pdf_id):\n",
    "        doc = fitz.open(stream=pdf_stream, filetype=\"pdf\")\n",
    "        data = []\n",
    "\n",
    "        MAIN_GROUPS = [\"ENGINEGROUP\", \"FRAMEGROUP\"]\n",
    "\n",
    "        section_pattern = r\"\\b((?:E|F|EOP)-\\d{1,3}(?:-\\d+)?)\\b\"\n",
    "\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text = page.get_text()\n",
    "            lines = text.splitlines()\n",
    "\n",
    "            # --- Check if page is a MAIN GROUP page ---\n",
    "            page_has_main_group = False\n",
    "\n",
    "            text_no_spaces = re.sub(r\"\\s+\", \"\", text).lower()\n",
    "\n",
    "            for group in MAIN_GROUPS:\n",
    "                if group.lower() in text_no_spaces:\n",
    "                    page_has_main_group = True\n",
    "                    break\n",
    "\n",
    "            if not page_has_main_group:\n",
    "                continue  # skip page\n",
    "\n",
    "            # --- Check if page has images ---\n",
    "            image_list = page.get_images()\n",
    "            if not image_list:\n",
    "                continue  # skip if no images\n",
    "\n",
    "            # --- Extract section labels from page ---\n",
    "            sections_found = []\n",
    "            for line in lines:\n",
    "                match = re.search(section_pattern, line)\n",
    "                if match:\n",
    "                    section = match.group(1)\n",
    "                    sections_found.append(section)\n",
    "\n",
    "            if not sections_found:\n",
    "                # print(f\"\\n=== PAGE {page_num+1} ===\")\n",
    "                # print(\"[SKIP] No sections found\")\n",
    "                continue\n",
    "            \n",
    "            # For debugging\n",
    "            # print(f\"\\n=== PAGE {page_num+1} ===\")\n",
    "            # print(f\"[MAIN GROUP PAGE] → {len(image_list)} image(s) found\")\n",
    "            # print(f\"Sections found: {sections_found}\")\n",
    "\n",
    "            # --- Map each section to corresponding image ---\n",
    "            # NOTE: assumes order of section labels = order of images\n",
    "            for idx, section in enumerate(sections_found):\n",
    "                if idx >= len(image_list):\n",
    "                    #print(f\"⚠️ Not enough images for sections — stopping at {idx}\")\n",
    "                    break\n",
    "\n",
    "                image_info = image_list[idx]\n",
    "                xref = image_info[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                image = normalize_image_background(base_image[\"image\"])\n",
    "\n",
    "                image_id = f\"{pdf_id}_{section}\"\n",
    "\n",
    "                data.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"pdf_id\": pdf_id,\n",
    "                    \"section\": section,\n",
    "                    \"image\": image\n",
    "                })\n",
    "\n",
    "                # # For debug: display the section + image\n",
    "                # img = Image.open(BytesIO(image))\n",
    "                # display(img)\n",
    "\n",
    "                #print(f\"[PAGE {page_num+1}] {section} → Image saved\")\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def extract_text(self):\n",
    "        df = self.extract_all_sections_one_pass(\n",
    "            pdf_id=self.pdf_id,\n",
    "            year=self.year,\n",
    "            brand=self.brand,\n",
    "            model=self.model,\n",
    "            batch_id=self.batch_id,\n",
    "            pdf_path=self.pdf_path\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def extract_images(self, engine):\n",
    "        with open(self.pdf_path, \"rb\") as f:\n",
    "            pdf_stream = f.read()\n",
    "        df = self.honda_extract_images_with_fig_labels(\n",
    "            pdf_stream=pdf_stream,\n",
    "            pdf_id=self.pdf_id\n",
    "        )\n",
    "        return df\n",
    "\n",
    "def extract_model(pdf_name):\n",
    "    # Extract model: start of filename, letters/numbers/spaces until a special character (', _)\n",
    "    match = re.match(r\"([A-Za-z0-9 ]+)\", pdf_name)\n",
    "    if match:\n",
    "        return match.group(1).replace(\" \", \"\")  # Removes any spaces\n",
    "def extract_batch_id(pdf_name, brand):\n",
    "    if brand == \"Yamaha\":\n",
    "        # Extract model codes inside parentheses\n",
    "        match = re.search(r\"\\((.*?)\\)\", pdf_name)\n",
    "        if match:\n",
    "            parts = match.group(1).split(\",\")\n",
    "            clean_parts = [part.strip() for part in parts]\n",
    "            return \"_\".join(clean_parts)\n",
    "    \n",
    "    elif brand == \"Honda\":\n",
    "        # Look for uppercase/digit code between underscores (6–10 characters)\n",
    "        match = re.search(r\"_([A-Z0-9]{6,10})_\", pdf_name)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "    return None\n",
    "def extract_year(pdf_name, brand):\n",
    "    if brand == \"Yamaha\":\n",
    "        year_match = re.search(r\"'(\\d{2})\", pdf_name)\n",
    "        return f\"20{year_match.group(1)}\" if year_match else None\n",
    "\n",
    "    # elif brand == \"Honda\":\n",
    "    #     match = re.search(r\"(20\\d{2}_20\\d{2})\", pdf_name)\n",
    "    #     return match.group(1) if match else None\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a3800f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HondaProcessor.honda_extract_images_with_fig_labels() got multiple values for argument 'pdf_stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     processor \u001b[38;5;241m=\u001b[39m HondaProcessor(pdf_file, pdf_id, brand, model, batch_id, year)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#df_text = processor.extract_text()\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m df_images \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_images\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "Cell \u001b[1;32mIn[25], line 643\u001b[0m, in \u001b[0;36mHondaProcessor.extract_images\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpdf_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    642\u001b[0m     pdf_stream \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 643\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhonda_extract_images_with_fig_labels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpdf_stream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdf_stream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpdf_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf_id\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[1;31mTypeError\u001b[0m: HondaProcessor.honda_extract_images_with_fig_labels() got multiple values for argument 'pdf_stream'"
     ]
    }
   ],
   "source": [
    "# Format 1: Yamaha\n",
    "pdf_1 = \"Manuals/AEROX 155 '19 (B65P, B65R, B65S).pdf\"\n",
    "pdf_2= \"Manuals/FJR1300A '15 (1MCH, 1MCG).PDF\"\n",
    "# Format 2: Honda\n",
    "pdf_3 = \"Manuals/CRF1000 A_PC_13MJPG02_(G.H).pdf\"\n",
    "pdf_4 = \"Manuals/NC750XAP_13MKWM02_PC_2022_2023.pdf\"\n",
    "\n",
    "pdf_file = pdf_4\n",
    "brand = \"Honda\"\n",
    "filename = os.path.basename(pdf_file)\n",
    "\n",
    "model = extract_model(filename)\n",
    "batch_id = extract_batch_id(filename, brand)\n",
    "year = extract_year(filename, brand)\n",
    "pdf_id = model + '_' + batch_id\n",
    "\n",
    "if brand == \"Yamaha\":\n",
    "    processor = YamahaProcessor(pdf_file, pdf_id, brand, model, batch_id, year)\n",
    "elif brand == \"Honda\":\n",
    "    processor = HondaProcessor(pdf_file, pdf_id, brand, model, batch_id, year)\n",
    "\n",
    "#df_text = processor.extract_text()\n",
    "df_images = processor.extract_images(engine)\n",
    "print(df_images.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d08a267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
